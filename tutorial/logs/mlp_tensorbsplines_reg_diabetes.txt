reg
diabetes

/home/mbaddar/Documents/mbaddar/phd/genmodel/genmodel39_venv/bin/python /home/mbaddar/Documents/mbaddar/phd/genmodel/RectifiedFlow/tutorial/test_tensor_bsplines_clf_reg.py
data range = (-0.13776722569000302, 0.19878798965729408)
########## Training MLP Regressor############
Iteration 1, loss = 14880.36420344
Iteration 2, loss = 14862.43884438
Iteration 3, loss = 14844.61956469
Iteration 4, loss = 14826.88536980
Iteration 5, loss = 14809.08859972
Iteration 6, loss = 14791.29067866
Iteration 7, loss = 14773.57487125
Iteration 8, loss = 14755.75662453
Iteration 9, loss = 14738.66522750
Iteration 10, loss = 14720.88499126
Iteration 11, loss = 14702.97848722
Iteration 12, loss = 14684.64389665
Iteration 13, loss = 14665.89717806
Iteration 14, loss = 14647.42619501
Iteration 15, loss = 14628.09879506
Iteration 16, loss = 14607.90295649
Iteration 17, loss = 14586.78806746
Iteration 18, loss = 14565.04423821
Iteration 19, loss = 14541.90859285
Iteration 20, loss = 14517.61068658
Iteration 21, loss = 14491.81287647
Iteration 22, loss = 14464.35885787
Iteration 23, loss = 14436.04307645
Iteration 24, loss = 14405.66266788
Iteration 25, loss = 14373.38333276
Iteration 26, loss = 14340.00541602
Iteration 27, loss = 14303.71819828
Iteration 28, loss = 14266.89896572
Iteration 29, loss = 14227.15821909
Iteration 30, loss = 14185.72966568
Iteration 31, loss = 14142.60691736
Iteration 32, loss = 14095.78393749
Iteration 33, loss = 14047.61067103
Iteration 34, loss = 13996.99414090
Iteration 35, loss = 13943.73115101
Iteration 36, loss = 13888.05497527
Iteration 37, loss = 13829.82601233
Iteration 38, loss = 13769.05394486
Iteration 39, loss = 13704.03478935
Iteration 40, loss = 13637.60875000
Iteration 41, loss = 13568.06469121
Iteration 42, loss = 13495.51671888
Iteration 43, loss = 13417.72359827
Iteration 44, loss = 13338.59213348
Iteration 45, loss = 13257.43635376
Iteration 46, loss = 13171.05689242
Iteration 47, loss = 13082.13493455
Iteration 48, loss = 12989.41048039
Iteration 49, loss = 12894.81431845
Iteration 50, loss = 12795.73660575
Iteration 51, loss = 12690.66018970
Iteration 52, loss = 12586.14520712
Iteration 53, loss = 12476.15981312
Iteration 54, loss = 12364.21927795
Iteration 55, loss = 12244.80866490
Iteration 56, loss = 12126.18896494
Iteration 57, loss = 12002.15166926
Iteration 58, loss = 11874.98283284
Iteration 59, loss = 11745.43247158
Iteration 60, loss = 11610.86868462
Iteration 61, loss = 11471.89098247
Iteration 62, loss = 11330.49072897
Iteration 63, loss = 11183.47112298
Iteration 64, loss = 11033.55329492
Iteration 65, loss = 10882.79648420
Iteration 66, loss = 10726.23958246
Iteration 67, loss = 10565.12949409
Iteration 68, loss = 10405.67640447
Iteration 69, loss = 10235.01282566
Iteration 70, loss = 10065.07180734
Iteration 71, loss = 9895.07305987
Iteration 72, loss = 9720.10241590
Iteration 73, loss = 9548.03834170
Iteration 74, loss = 9365.46101452
Iteration 75, loss = 9181.99257539
Iteration 76, loss = 9000.84549933
Iteration 77, loss = 8812.63394612
Iteration 78, loss = 8626.50821753
Iteration 79, loss = 8437.69756966
Iteration 80, loss = 8250.17922545
Iteration 81, loss = 8059.16275950
Iteration 82, loss = 7864.66197946
Iteration 83, loss = 7674.85989359
Iteration 84, loss = 7481.47749523
Iteration 85, loss = 7290.15135248
Iteration 86, loss = 7095.56226970
Iteration 87, loss = 6907.87041308
Iteration 88, loss = 6715.20143144
Iteration 89, loss = 6520.74253538
Iteration 90, loss = 6337.99503240
Iteration 91, loss = 6147.16339120
Iteration 92, loss = 5957.89204899
Iteration 93, loss = 5776.18267966
Iteration 94, loss = 5601.24442388
Iteration 95, loss = 5417.33470811
Iteration 96, loss = 5244.02683780
Iteration 97, loss = 5071.85691096
Iteration 98, loss = 4908.01612355
Iteration 99, loss = 4740.09583367
Iteration 100, loss = 4581.52581329
Iteration 101, loss = 4425.33368100
Iteration 102, loss = 4280.45397966
Iteration 103, loss = 4129.28845447
Iteration 104, loss = 3992.06636988
Iteration 105, loss = 3855.87534991
Iteration 106, loss = 3726.91184050
Iteration 107, loss = 3603.29096557
Iteration 108, loss = 3484.02532903
Iteration 109, loss = 3370.81218451
Iteration 110, loss = 3260.35902689
Iteration 111, loss = 3161.49359744
Iteration 112, loss = 3065.78910463
Iteration 113, loss = 2975.51210098
Iteration 114, loss = 2887.92532204
Iteration 115, loss = 2811.74805310
Iteration 116, loss = 2735.57030851
Iteration 117, loss = 2667.76908412
Iteration 118, loss = 2600.17732363
Iteration 119, loss = 2541.61347905
Iteration 120, loss = 2488.70757714
Iteration 121, loss = 2438.42595234
Iteration 122, loss = 2390.01055695
Iteration 123, loss = 2351.05920600
Iteration 124, loss = 2312.07765183
Iteration 125, loss = 2277.84911069
Iteration 126, loss = 2247.15982679
Iteration 127, loss = 2217.92933837
Iteration 128, loss = 2193.80384164
Iteration 129, loss = 2168.70570425
Iteration 130, loss = 2150.37747913
Iteration 131, loss = 2132.14073311
Iteration 132, loss = 2116.25382493
Iteration 133, loss = 2103.29067234
Iteration 134, loss = 2090.48771229
Iteration 135, loss = 2077.14971614
Iteration 136, loss = 2067.90141522
Iteration 137, loss = 2059.34171358
Iteration 138, loss = 2049.58106346
Iteration 139, loss = 2041.24308408
Iteration 140, loss = 2034.79843197
Iteration 141, loss = 2030.15278787
Iteration 142, loss = 2023.28470753
Iteration 143, loss = 2018.02284829
Iteration 144, loss = 2012.04254876
Iteration 145, loss = 2007.18792443
Iteration 146, loss = 2002.09423454
Iteration 147, loss = 1997.57774336
Iteration 148, loss = 1992.93290248
Iteration 149, loss = 1988.73649112
Iteration 150, loss = 1984.76226702
Iteration 151, loss = 1981.04760682
Iteration 152, loss = 1976.39016581
Iteration 153, loss = 1972.27846654
Iteration 154, loss = 1968.58069258
Iteration 155, loss = 1964.38795556
Iteration 156, loss = 1960.81001877
Iteration 157, loss = 1956.87773265
Iteration 158, loss = 1952.85952914
Iteration 159, loss = 1949.10903900
Iteration 160, loss = 1945.53711305
Iteration 161, loss = 1941.65379180
Iteration 162, loss = 1938.07421998
Iteration 163, loss = 1934.55222498
Iteration 164, loss = 1930.90712414
Iteration 165, loss = 1927.41117337
Iteration 166, loss = 1923.60450281
Iteration 167, loss = 1920.32629365
Iteration 168, loss = 1916.65980992
Iteration 169, loss = 1913.33687256
Iteration 170, loss = 1909.81636310
Iteration 171, loss = 1906.40398273
Iteration 172, loss = 1903.06774783
Iteration 173, loss = 1899.84445562
Iteration 174, loss = 1896.31195151
Iteration 175, loss = 1893.13391660
Iteration 176, loss = 1889.72178473
Iteration 177, loss = 1886.57183247
Iteration 178, loss = 1883.49399852
Iteration 179, loss = 1880.31078529
Iteration 180, loss = 1876.96612486
Iteration 181, loss = 1873.86414654
Iteration 182, loss = 1870.69034947
Iteration 183, loss = 1867.76818901
Iteration 184, loss = 1864.51702698
Iteration 185, loss = 1861.50297084
Iteration 186, loss = 1858.57451684
Iteration 187, loss = 1855.47985923
Iteration 188, loss = 1852.69552320
Iteration 189, loss = 1849.70025031
Iteration 190, loss = 1846.66345012
Iteration 191, loss = 1843.70329876
Iteration 192, loss = 1840.90765595
Iteration 193, loss = 1838.01000347
Iteration 194, loss = 1835.26079719
Iteration 195, loss = 1832.41557038
Iteration 196, loss = 1829.51531124
Iteration 197, loss = 1826.81715464
Iteration 198, loss = 1824.07747034
Iteration 199, loss = 1821.42441593
Iteration 200, loss = 1818.57163866
Iteration 201, loss = 1815.96351783
Iteration 202, loss = 1813.35422676
Iteration 203, loss = 1810.45907611
Iteration 204, loss = 1807.95044790
Iteration 205, loss = 1805.47393858
Iteration 206, loss = 1802.72346796
Iteration 207, loss = 1800.17322238
Iteration 208, loss = 1797.57948742
Iteration 209, loss = 1795.10292985
Iteration 210, loss = 1792.49896463
Iteration 211, loss = 1789.95486532
Iteration 212, loss = 1787.41453062
Iteration 213, loss = 1784.99784839
Iteration 214, loss = 1782.64295478
Iteration 215, loss = 1779.91487991
Iteration 216, loss = 1777.30456652
Iteration 217, loss = 1775.07814806
Iteration 218, loss = 1772.71090538
Iteration 219, loss = 1770.29978150
Iteration 220, loss = 1768.08712088
Iteration 221, loss = 1765.38947111
Iteration 222, loss = 1763.08241957
Iteration 223, loss = 1760.77890447
Iteration 224, loss = 1758.43033155
Iteration 225, loss = 1756.02393032
Iteration 226, loss = 1753.76496621
Iteration 227, loss = 1751.56665503
Iteration 228, loss = 1749.30893852
Iteration 229, loss = 1747.01995976
Iteration 230, loss = 1744.85794155
Iteration 231, loss = 1742.48830968
Iteration 232, loss = 1740.33882669
Iteration 233, loss = 1738.11278238
Iteration 234, loss = 1735.85072672
Iteration 235, loss = 1733.77266559
Iteration 236, loss = 1731.64360575
Iteration 237, loss = 1729.39412015
Iteration 238, loss = 1727.50251874
Iteration 239, loss = 1725.34864240
Iteration 240, loss = 1723.05391739
Iteration 241, loss = 1721.00038643
Iteration 242, loss = 1719.02790183
Iteration 243, loss = 1716.95396199
Iteration 244, loss = 1714.95316780
Iteration 245, loss = 1712.78184892
Iteration 246, loss = 1710.91995326
Iteration 247, loss = 1708.79580188
Iteration 248, loss = 1706.79702972
Iteration 249, loss = 1704.77431088
Iteration 250, loss = 1702.80616382
Iteration 251, loss = 1701.04293735
Iteration 252, loss = 1699.04687728
Iteration 253, loss = 1697.10611254
Iteration 254, loss = 1695.17091065
Iteration 255, loss = 1693.30251352
Iteration 256, loss = 1691.43074681
Iteration 257, loss = 1689.57389504
Iteration 258, loss = 1687.71680504
Iteration 259, loss = 1685.87125548
Iteration 260, loss = 1684.07259602
Iteration 261, loss = 1682.15433214
Iteration 262, loss = 1680.38411756
Iteration 263, loss = 1678.63782698
Iteration 264, loss = 1676.80475815
Iteration 265, loss = 1675.06546487
Iteration 266, loss = 1673.45544012
Iteration 267, loss = 1671.66181714
Iteration 268, loss = 1670.00114223
Iteration 269, loss = 1668.15534199
Iteration 270, loss = 1666.50257961
Iteration 271, loss = 1664.90426452
Iteration 272, loss = 1663.16147869
Iteration 273, loss = 1661.55109304
Iteration 274, loss = 1659.91510622
Iteration 275, loss = 1658.17772361
Iteration 276, loss = 1656.65109722
Iteration 277, loss = 1654.96521287
Iteration 278, loss = 1653.48409289
Iteration 279, loss = 1652.00334461
Iteration 280, loss = 1650.20851239
Iteration 281, loss = 1648.71659224
Iteration 282, loss = 1647.09756715
Iteration 283, loss = 1645.57983173
Iteration 284, loss = 1644.17941987
Iteration 285, loss = 1642.45848644
Iteration 286, loss = 1641.04470056
Iteration 287, loss = 1639.56978301
Iteration 288, loss = 1638.01839599
Iteration 289, loss = 1636.49133535
Iteration 290, loss = 1635.01417632
Iteration 291, loss = 1633.55603927
Iteration 292, loss = 1632.17179483
Iteration 293, loss = 1630.62431987
Iteration 294, loss = 1629.32720661
Iteration 295, loss = 1627.78220945
Iteration 296, loss = 1626.38593995
Iteration 297, loss = 1625.19215815
Iteration 298, loss = 1623.53975191
Iteration 299, loss = 1622.13257374
Iteration 300, loss = 1620.82325817
Iteration 301, loss = 1619.36046699
Iteration 302, loss = 1618.18545178
Iteration 303, loss = 1617.16764685
Iteration 304, loss = 1615.45502741
Iteration 305, loss = 1614.03741626
Iteration 306, loss = 1612.97412241
Iteration 307, loss = 1611.57824129
Iteration 308, loss = 1610.23012455
Iteration 309, loss = 1608.80784157
Iteration 310, loss = 1607.65014727
Iteration 311, loss = 1606.46849859
Iteration 312, loss = 1605.09724122
Iteration 313, loss = 1603.99186873
Iteration 314, loss = 1602.72886670
Iteration 315, loss = 1601.62872342
Iteration 316, loss = 1600.33855459
Iteration 317, loss = 1599.04806483
Iteration 318, loss = 1597.89100629
Iteration 319, loss = 1596.68865923
Iteration 320, loss = 1595.67133272
Iteration 321, loss = 1594.47386308
Iteration 322, loss = 1593.30711931
Iteration 323, loss = 1592.19762948
Iteration 324, loss = 1590.96447236
Iteration 325, loss = 1589.89520685
Iteration 326, loss = 1588.76841428
Iteration 327, loss = 1587.55767169
Iteration 328, loss = 1586.54364000
Iteration 329, loss = 1585.57067776
Iteration 330, loss = 1584.38587128
Iteration 331, loss = 1583.24506755
Iteration 332, loss = 1582.30451307
Iteration 333, loss = 1581.21230043
Iteration 334, loss = 1579.98531988
Iteration 335, loss = 1579.02246327
Iteration 336, loss = 1577.91130632
Iteration 337, loss = 1576.98740480
Iteration 338, loss = 1575.91081022
Iteration 339, loss = 1574.95009329
Iteration 340, loss = 1573.84691131
Iteration 341, loss = 1573.04997892
Iteration 342, loss = 1571.81699883
Iteration 343, loss = 1570.90601083
Iteration 344, loss = 1569.93829787
Iteration 345, loss = 1568.94505384
Iteration 346, loss = 1568.02324279
Iteration 347, loss = 1567.06143457
Iteration 348, loss = 1566.11535180
Iteration 349, loss = 1565.25341252
Iteration 350, loss = 1564.22287663
Iteration 351, loss = 1563.27783895
Iteration 352, loss = 1562.62332835
Iteration 353, loss = 1561.40151644
Iteration 354, loss = 1560.54739607
Iteration 355, loss = 1559.59104245
Iteration 356, loss = 1558.89702421
Iteration 357, loss = 1557.88160927
Iteration 358, loss = 1557.06109851
Iteration 359, loss = 1556.26450175
Iteration 360, loss = 1555.36735565
Iteration 361, loss = 1554.55182800
Iteration 362, loss = 1553.61527360
Iteration 363, loss = 1552.78890192
Iteration 364, loss = 1551.98267704
Iteration 365, loss = 1551.15464555
Iteration 366, loss = 1550.35739818
Iteration 367, loss = 1549.56219030
Iteration 368, loss = 1548.72479400
Iteration 369, loss = 1548.05667830
Iteration 370, loss = 1547.18092426
Iteration 371, loss = 1546.54565540
Iteration 372, loss = 1545.65873152
Iteration 373, loss = 1544.91738280
Iteration 374, loss = 1544.07971639
Iteration 375, loss = 1543.41256110
Iteration 376, loss = 1542.64067005
Iteration 377, loss = 1541.86310213
Iteration 378, loss = 1541.10643054
Iteration 379, loss = 1540.43807867
Iteration 380, loss = 1539.58533072
Iteration 381, loss = 1538.98579439
Iteration 382, loss = 1538.32173250
Iteration 383, loss = 1537.50584318
Iteration 384, loss = 1536.87567531
Iteration 385, loss = 1536.34587189
Iteration 386, loss = 1535.39509623
Iteration 387, loss = 1534.80645856
Iteration 388, loss = 1534.15941073
Iteration 389, loss = 1533.47673030
Iteration 390, loss = 1532.75816208
Iteration 391, loss = 1532.06712591
Iteration 392, loss = 1531.54007725
Iteration 393, loss = 1530.72989787
Iteration 394, loss = 1530.16410457
Iteration 395, loss = 1529.54652730
Iteration 396, loss = 1528.98874317
Iteration 397, loss = 1528.21129424
Iteration 398, loss = 1527.61001170
Iteration 399, loss = 1527.05131210
Iteration 400, loss = 1526.37045926
Iteration 401, loss = 1525.79636077
Iteration 402, loss = 1525.24526667
Iteration 403, loss = 1524.55218532
Iteration 404, loss = 1524.02021736
Iteration 405, loss = 1523.57063474
Iteration 406, loss = 1522.84652501
Iteration 407, loss = 1522.22339027
Iteration 408, loss = 1521.64223792
Iteration 409, loss = 1521.16910111
Iteration 410, loss = 1520.56279044
Iteration 411, loss = 1519.96115137
Iteration 412, loss = 1519.48731133
Iteration 413, loss = 1518.93139427
Iteration 414, loss = 1518.31006395
Iteration 415, loss = 1517.81526400
Iteration 416, loss = 1517.27605280
Iteration 417, loss = 1516.77756507
Iteration 418, loss = 1516.20124175
Iteration 419, loss = 1515.67058780
Iteration 420, loss = 1515.18513196
Iteration 421, loss = 1514.82584937
Iteration 422, loss = 1514.20972007
Iteration 423, loss = 1513.65030122
Iteration 424, loss = 1513.25542606
Iteration 425, loss = 1512.72200489
Iteration 426, loss = 1512.18421986
Iteration 427, loss = 1511.70058949
Iteration 428, loss = 1511.35731925
Iteration 429, loss = 1510.90817422
Iteration 430, loss = 1510.29343340
Iteration 431, loss = 1509.86338504
Iteration 432, loss = 1509.43715963
Iteration 433, loss = 1509.01097229
Iteration 434, loss = 1508.49309305
Iteration 435, loss = 1508.07147130
Iteration 436, loss = 1507.61658779
Iteration 437, loss = 1507.36630158
Iteration 438, loss = 1506.69003897
Iteration 439, loss = 1506.22343413
Iteration 440, loss = 1505.83982813
Iteration 441, loss = 1505.38087729
Iteration 442, loss = 1505.00158060
Iteration 443, loss = 1504.62205106
Iteration 444, loss = 1504.14476070
Iteration 445, loss = 1503.76328095
Iteration 446, loss = 1503.26959104
Iteration 447, loss = 1502.98352584
Iteration 448, loss = 1502.54254564
Iteration 449, loss = 1502.05377613
Iteration 450, loss = 1501.69125665
Iteration 451, loss = 1501.30840666
Iteration 452, loss = 1500.92286775
Iteration 453, loss = 1500.58504100
Iteration 454, loss = 1500.13058281
Iteration 455, loss = 1499.81729568
Iteration 456, loss = 1499.82976563
Iteration 457, loss = 1499.01636513
Iteration 458, loss = 1498.58292762
Iteration 459, loss = 1498.16728266
Iteration 460, loss = 1497.79281665
Iteration 461, loss = 1497.42542296
Iteration 462, loss = 1497.11011341
Iteration 463, loss = 1496.69145871
Iteration 464, loss = 1496.36182385
Iteration 465, loss = 1496.07310958
Iteration 466, loss = 1495.62293665
Iteration 467, loss = 1495.34158985
Iteration 468, loss = 1494.90236949
Iteration 469, loss = 1494.59018516
Iteration 470, loss = 1494.26374258
Iteration 471, loss = 1493.93437483
Iteration 472, loss = 1493.56412470
Iteration 473, loss = 1493.28804427
Iteration 474, loss = 1492.91005196
Iteration 475, loss = 1492.50729742
Iteration 476, loss = 1492.27114416
Iteration 477, loss = 1491.93703555
Iteration 478, loss = 1491.59335063
Iteration 479, loss = 1491.28685313
Iteration 480, loss = 1490.95078633
Iteration 481, loss = 1490.63876730
Iteration 482, loss = 1490.28939817
Iteration 483, loss = 1490.03417067
Iteration 484, loss = 1489.72266969
Iteration 485, loss = 1489.39690380
Iteration 486, loss = 1489.13495592
Iteration 487, loss = 1488.85505285
Iteration 488, loss = 1488.59114784
Iteration 489, loss = 1488.18783496
Iteration 490, loss = 1487.83113203
Iteration 491, loss = 1487.63950803
Iteration 492, loss = 1487.45277494
Iteration 493, loss = 1487.06337268
Iteration 494, loss = 1486.97730654
Iteration 495, loss = 1486.59715672
Iteration 496, loss = 1486.29702518
Iteration 497, loss = 1486.00155682
Iteration 498, loss = 1485.71314784
Iteration 499, loss = 1485.46151480
Iteration 500, loss = 1485.18343295
Iteration 501, loss = 1484.91061025
Iteration 502, loss = 1484.61123709
Iteration 503, loss = 1484.31389549
Iteration 504, loss = 1484.10912852
Iteration 505, loss = 1483.91244914
Iteration 506, loss = 1483.59329153
Iteration 507, loss = 1483.39107197
Iteration 508, loss = 1483.14718459
Iteration 509, loss = 1482.89591507
Iteration 510, loss = 1482.65337310
Iteration 511, loss = 1482.33306123
Iteration 512, loss = 1482.11145724
Iteration 513, loss = 1481.85949674
Iteration 514, loss = 1481.69224879
Iteration 515, loss = 1481.38368075
Iteration 516, loss = 1481.17627097
Iteration 517, loss = 1480.87670703
Iteration 518, loss = 1480.69524989
Iteration 519, loss = 1480.47032102
Iteration 520, loss = 1480.24686558
Iteration 521, loss = 1480.15251765
Iteration 522, loss = 1479.76606666
Iteration 523, loss = 1479.56684936
Iteration 524, loss = 1479.33024380
Iteration 525, loss = 1479.12032096
Iteration 526, loss = 1479.06853535
Iteration 527, loss = 1478.65954780
Iteration 528, loss = 1478.47048596
Iteration 529, loss = 1478.30318964
Iteration 530, loss = 1478.08567935
Iteration 531, loss = 1478.11162152
Iteration 532, loss = 1477.64174046
Iteration 533, loss = 1477.49505083
Iteration 534, loss = 1477.25226558
Iteration 535, loss = 1477.04705229
Iteration 536, loss = 1476.83745543
Iteration 537, loss = 1476.67510358
Iteration 538, loss = 1476.48963718
Iteration 539, loss = 1476.21304034
Iteration 540, loss = 1476.02854685
Iteration 541, loss = 1475.82845775
Iteration 542, loss = 1475.68253576
Iteration 543, loss = 1475.47641640
Iteration 544, loss = 1475.28319510
Iteration 545, loss = 1475.05030994
Iteration 546, loss = 1474.95620289
Iteration 547, loss = 1474.68621209
Iteration 548, loss = 1474.77891619
Iteration 549, loss = 1474.34811057
Iteration 550, loss = 1474.13544522
Iteration 551, loss = 1474.01679370
Iteration 552, loss = 1473.80053280
Iteration 553, loss = 1473.59402742
Iteration 554, loss = 1473.44852008
Iteration 555, loss = 1473.26039928
Iteration 556, loss = 1473.06237018
Iteration 557, loss = 1472.93706988
Iteration 558, loss = 1472.71257979
Iteration 559, loss = 1472.53165764
Iteration 560, loss = 1472.36213740
Iteration 561, loss = 1472.25363170
Iteration 562, loss = 1472.05081013
Iteration 563, loss = 1471.90247302
Iteration 564, loss = 1471.71783560
Iteration 565, loss = 1471.59673416
Iteration 566, loss = 1471.42941698
Iteration 567, loss = 1471.25087983
Iteration 568, loss = 1471.07796007
Iteration 569, loss = 1471.02266989
Iteration 570, loss = 1470.78174380
Iteration 571, loss = 1470.61504305
Iteration 572, loss = 1470.50598123
Iteration 573, loss = 1470.32303533
Iteration 574, loss = 1470.18668481
Iteration 575, loss = 1470.01873286
Iteration 576, loss = 1469.86340659
Iteration 577, loss = 1469.74858204
Iteration 578, loss = 1469.55018869
Iteration 579, loss = 1469.45075103
Iteration 580, loss = 1469.27757219
Iteration 581, loss = 1469.12461441
Iteration 582, loss = 1469.04713201
Iteration 583, loss = 1468.89076340
Iteration 584, loss = 1468.69175582
Iteration 585, loss = 1468.56465590
Iteration 586, loss = 1468.49067497
Iteration 587, loss = 1468.29773722
Iteration 588, loss = 1468.21602902
Iteration 589, loss = 1467.99138195
Iteration 590, loss = 1467.94935087
Iteration 591, loss = 1467.74374475
Iteration 592, loss = 1467.56690158
Iteration 593, loss = 1467.46153694
Iteration 594, loss = 1467.30726082
Iteration 595, loss = 1467.17971489
Iteration 596, loss = 1467.06677380
Iteration 597, loss = 1466.92955775
Iteration 598, loss = 1466.85039796
Iteration 599, loss = 1466.73044569
Iteration 600, loss = 1466.54221156
Iteration 601, loss = 1466.42149300
Iteration 602, loss = 1466.43769467
Iteration 603, loss = 1466.14407866
Iteration 604, loss = 1466.05229726
Iteration 605, loss = 1465.88604785
Iteration 606, loss = 1465.78217708
Iteration 607, loss = 1465.68296359
Iteration 608, loss = 1465.51856986
Iteration 609, loss = 1465.41481535
Iteration 610, loss = 1465.26820940
Iteration 611, loss = 1465.14407328
Iteration 612, loss = 1464.99387901
Iteration 613, loss = 1464.88667332
Iteration 614, loss = 1464.78947094
Iteration 615, loss = 1464.72604018
Iteration 616, loss = 1464.54516642
Iteration 617, loss = 1464.43054639
Iteration 618, loss = 1464.35716619
Iteration 619, loss = 1464.16522246
Iteration 620, loss = 1464.09262698
Iteration 621, loss = 1464.00951427
Iteration 622, loss = 1463.81735439
Iteration 623, loss = 1463.72670190
Iteration 624, loss = 1463.62187925
Iteration 625, loss = 1463.54025997
Iteration 626, loss = 1463.46478079
Iteration 627, loss = 1463.29944499
Iteration 628, loss = 1463.18128387
Iteration 629, loss = 1463.16350919
Iteration 630, loss = 1462.95637416
Iteration 631, loss = 1462.88043083
Iteration 632, loss = 1462.73506817
Iteration 633, loss = 1462.65106812
Iteration 634, loss = 1462.53330908
Iteration 635, loss = 1462.52559533
Iteration 636, loss = 1462.38929983
Iteration 637, loss = 1462.31829776
Iteration 638, loss = 1462.19602749
Iteration 639, loss = 1462.01303363
Iteration 640, loss = 1462.01307338
Iteration 641, loss = 1462.03469199
Iteration 642, loss = 1461.77077611
Iteration 643, loss = 1461.73217813
Iteration 644, loss = 1461.56160586
Iteration 645, loss = 1461.52752852
Iteration 646, loss = 1461.35563024
Iteration 647, loss = 1461.31792148
Iteration 648, loss = 1461.18315061
Iteration 649, loss = 1461.08593342
Iteration 650, loss = 1460.96058628
Iteration 651, loss = 1460.86444196
Iteration 652, loss = 1460.80028893
Iteration 653, loss = 1460.66925297
Iteration 654, loss = 1460.55733629
Iteration 655, loss = 1460.50606770
Iteration 656, loss = 1460.45868414
Iteration 657, loss = 1460.42607653
Iteration 658, loss = 1460.37095280
Iteration 659, loss = 1460.09798133
Iteration 660, loss = 1461.00288420
Iteration 661, loss = 1459.95580952
Iteration 662, loss = 1459.80584106
Iteration 663, loss = 1459.80103670
Iteration 664, loss = 1459.65215552
Iteration 665, loss = 1459.61167093
Iteration 666, loss = 1459.44810024
Iteration 667, loss = 1459.40367439
Iteration 668, loss = 1459.29962210
Iteration 669, loss = 1459.20474768
Iteration 670, loss = 1459.19149466
Iteration 671, loss = 1459.02396353
Iteration 672, loss = 1459.18788281
Iteration 673, loss = 1459.07074200
Iteration 674, loss = 1458.78449336
Iteration 675, loss = 1458.75297823
Iteration 676, loss = 1458.67347611
Iteration 677, loss = 1458.55829845
Iteration 678, loss = 1458.77024851
Iteration 679, loss = 1458.35772792
Iteration 680, loss = 1458.35785040
Iteration 681, loss = 1458.26820706
Iteration 682, loss = 1458.51691912
Iteration 683, loss = 1458.12907170
Iteration 684, loss = 1458.04276344
Iteration 685, loss = 1457.82785395
Iteration 686, loss = 1457.82747988
Iteration 687, loss = 1457.77974786
Iteration 688, loss = 1457.66127731
Iteration 689, loss = 1457.66186223
Iteration 690, loss = 1457.56436012
Iteration 691, loss = 1457.44203213
Iteration 692, loss = 1457.37206182
Iteration 693, loss = 1457.37164436
Iteration 694, loss = 1457.18958419
Iteration 695, loss = 1457.17507453
Iteration 696, loss = 1457.37766781
Iteration 697, loss = 1457.01055688
Iteration 698, loss = 1456.89403408
Iteration 699, loss = 1457.10815629
Iteration 700, loss = 1456.79798753
Iteration 701, loss = 1456.84537095
Iteration 702, loss = 1456.65737007
Iteration 703, loss = 1456.63483386
Iteration 704, loss = 1456.48297362
Iteration 705, loss = 1456.41344824
Iteration 706, loss = 1456.42545956
Iteration 707, loss = 1456.46281519
Iteration 708, loss = 1456.25751095
Iteration 709, loss = 1456.13019811
Iteration 710, loss = 1456.04177619
Iteration 711, loss = 1456.04324511
Iteration 712, loss = 1456.02562996
Iteration 713, loss = 1455.87846321
Iteration 714, loss = 1455.80921456
Iteration 715, loss = 1455.74744405
Iteration 716, loss = 1455.67371560
Iteration 717, loss = 1455.64199462
Iteration 718, loss = 1455.50761669
Iteration 719, loss = 1455.52342634
Iteration 720, loss = 1455.49513053
Iteration 721, loss = 1455.34420172
Iteration 722, loss = 1455.27786046
Iteration 723, loss = 1455.22491577
Iteration 724, loss = 1455.24107867
Iteration 725, loss = 1455.08769052
Iteration 726, loss = 1455.14145724
Iteration 727, loss = 1454.98458908
Iteration 728, loss = 1454.97760890
Iteration 729, loss = 1454.83642155
Iteration 730, loss = 1454.80371177
Iteration 731, loss = 1454.76899686
Iteration 732, loss = 1454.65253513
Iteration 733, loss = 1454.56736981
Iteration 734, loss = 1454.70291336
Iteration 735, loss = 1454.45380220
Iteration 736, loss = 1454.40163653
Iteration 737, loss = 1454.39368829
Iteration 738, loss = 1454.43386288
Iteration 739, loss = 1454.33854965
Iteration 740, loss = 1454.30976547
Iteration 741, loss = 1454.11296066
Iteration 742, loss = 1454.07045308
Iteration 743, loss = 1453.99900827
Iteration 744, loss = 1453.95509507
Iteration 745, loss = 1453.89209381
Iteration 746, loss = 1453.99245710
Iteration 747, loss = 1453.73550922
Iteration 748, loss = 1453.65404523
Iteration 749, loss = 1453.87374748
Iteration 750, loss = 1453.59043844
Iteration 751, loss = 1453.62049704
Iteration 752, loss = 1453.46474654
Iteration 753, loss = 1453.65455246
Iteration 754, loss = 1453.30599015
Iteration 755, loss = 1453.35692686
Iteration 756, loss = 1453.30858946
Iteration 757, loss = 1453.13202316
Iteration 758, loss = 1453.17102451
Iteration 759, loss = 1453.06444402
Iteration 760, loss = 1453.03754143
Iteration 761, loss = 1453.04623691
Iteration 762, loss = 1452.95290696
Iteration 763, loss = 1452.95873918
Iteration 764, loss = 1452.93965615
Iteration 765, loss = 1452.80020149
Iteration 766, loss = 1452.67309595
Iteration 767, loss = 1452.64946587
Iteration 768, loss = 1452.68421010
Iteration 769, loss = 1452.50849002
Iteration 770, loss = 1452.58592315
Iteration 771, loss = 1452.41577457
Iteration 772, loss = 1452.37040503
Iteration 773, loss = 1452.36200627
Iteration 774, loss = 1452.25177390
Iteration 775, loss = 1452.32949177
Iteration 776, loss = 1452.40074173
Iteration 777, loss = 1452.14577064
Iteration 778, loss = 1452.06871421
Iteration 779, loss = 1452.46293632
Iteration 780, loss = 1451.98058666
Iteration 781, loss = 1452.02990877
Iteration 782, loss = 1451.92795714
Iteration 783, loss = 1451.91739218
Iteration 784, loss = 1451.77333161
Iteration 785, loss = 1451.74165373
Iteration 786, loss = 1451.72228302
Iteration 787, loss = 1451.67267350
Iteration 788, loss = 1451.59888956
Iteration 789, loss = 1451.58113254
Iteration 790, loss = 1451.51202346
Iteration 791, loss = 1451.47114131
Iteration 792, loss = 1451.46268664
Iteration 793, loss = 1451.37067815
Iteration 794, loss = 1451.36469276
Iteration 795, loss = 1451.24159249
Iteration 796, loss = 1451.27758314
Iteration 797, loss = 1451.14257496
Iteration 798, loss = 1451.18813248
Iteration 799, loss = 1451.11708142
Iteration 800, loss = 1451.25021368
Iteration 801, loss = 1451.13899470
Iteration 802, loss = 1451.12873342
Iteration 803, loss = 1451.06338269
Iteration 804, loss = 1450.96276685
Iteration 805, loss = 1451.00298326
Iteration 806, loss = 1450.80543703
Iteration 807, loss = 1450.76286912
Iteration 808, loss = 1450.71563602
Iteration 809, loss = 1450.84762891
Iteration 810, loss = 1450.59999186
Iteration 811, loss = 1450.60844136
Iteration 812, loss = 1450.69369968
Iteration 813, loss = 1450.69603006
Iteration 814, loss = 1450.45088339
Iteration 815, loss = 1450.49067388
Iteration 816, loss = 1450.58136612
Iteration 817, loss = 1450.39247135
Iteration 818, loss = 1450.24132066
Iteration 819, loss = 1450.24387003
Iteration 820, loss = 1450.14540587
Iteration 821, loss = 1450.24579045
Iteration 822, loss = 1450.05771351
Iteration 823, loss = 1450.01166524
Iteration 824, loss = 1450.13917048
Iteration 825, loss = 1450.05471389
Iteration 826, loss = 1450.04837901
Iteration 827, loss = 1449.92672777
Iteration 828, loss = 1449.81422241
Iteration 829, loss = 1449.78804502
Iteration 830, loss = 1449.71930673
Iteration 831, loss = 1450.01667379
Iteration 832, loss = 1449.66743039
Iteration 833, loss = 1449.59298765
Iteration 834, loss = 1449.59967426
Iteration 835, loss = 1449.51883669
Iteration 836, loss = 1449.46480198
Iteration 837, loss = 1449.48681521
Iteration 838, loss = 1449.49410490
Iteration 839, loss = 1449.41504458
Iteration 840, loss = 1449.35663276
Iteration 841, loss = 1449.34693590
Iteration 842, loss = 1449.35780494
Iteration 843, loss = 1449.23570903
Iteration 844, loss = 1449.50219353
Iteration 845, loss = 1449.21828357
Iteration 846, loss = 1449.23453692
Iteration 847, loss = 1449.08487678
Iteration 848, loss = 1449.03418257
Iteration 849, loss = 1449.04649271
Iteration 850, loss = 1448.97649593
Iteration 851, loss = 1448.92552279
Iteration 852, loss = 1448.96208276
Iteration 853, loss = 1448.84472030
Iteration 854, loss = 1448.84310999
Iteration 855, loss = 1448.78867485
Iteration 856, loss = 1448.76524643
Iteration 857, loss = 1448.77226588
Iteration 858, loss = 1448.73143674
Iteration 859, loss = 1448.62757613
Iteration 860, loss = 1448.70854125
Iteration 861, loss = 1448.69407796
Iteration 862, loss = 1448.52016198
Iteration 863, loss = 1448.49390909
Iteration 864, loss = 1448.44785073
Iteration 865, loss = 1448.43345014
Iteration 866, loss = 1448.43736846
Iteration 867, loss = 1448.43224239
Iteration 868, loss = 1448.36110030
Iteration 869, loss = 1448.33641051
Iteration 870, loss = 1448.38622275
Iteration 871, loss = 1448.26511084
Iteration 872, loss = 1448.17081746
Iteration 873, loss = 1448.14655424
Iteration 874, loss = 1448.16517397
Iteration 875, loss = 1448.08506577
Iteration 876, loss = 1448.02940192
Iteration 877, loss = 1447.95756355
Iteration 878, loss = 1448.02406251
Iteration 879, loss = 1447.97737164
Iteration 880, loss = 1447.84994778
Iteration 881, loss = 1447.83337845
Iteration 882, loss = 1447.95481605
Iteration 883, loss = 1447.86248083
Iteration 884, loss = 1447.85484050
Iteration 885, loss = 1447.98370446
Iteration 886, loss = 1447.69467619
Iteration 887, loss = 1447.65679563
Iteration 888, loss = 1447.60662376
Iteration 889, loss = 1447.59518506
Iteration 890, loss = 1447.53077593
Iteration 891, loss = 1447.56218981
Iteration 892, loss = 1447.66216884
Iteration 893, loss = 1447.50290226
Iteration 894, loss = 1447.48445326
Iteration 895, loss = 1447.55030920
Iteration 896, loss = 1447.57208216
Iteration 897, loss = 1447.32842990
Iteration 898, loss = 1447.32930615
Iteration 899, loss = 1447.38709010
Iteration 900, loss = 1447.24194752
Iteration 901, loss = 1447.21790905
Iteration 902, loss = 1447.33919330
Iteration 903, loss = 1447.13831730
Iteration 904, loss = 1447.12760470
Iteration 905, loss = 1447.12270304
Iteration 906, loss = 1447.04572834
Iteration 907, loss = 1447.04089478
Iteration 908, loss = 1447.06621518
Iteration 909, loss = 1447.01771980
Iteration 910, loss = 1446.95622175
Iteration 911, loss = 1447.01482621
Iteration 912, loss = 1446.91776070
Iteration 913, loss = 1446.86776073
Iteration 914, loss = 1446.87402533
Iteration 915, loss = 1446.82562180
Iteration 916, loss = 1446.86237879
Iteration 917, loss = 1446.74724237
Iteration 918, loss = 1446.71374664
Iteration 919, loss = 1446.91257236
Iteration 920, loss = 1446.70961517
Iteration 921, loss = 1446.64376181
Iteration 922, loss = 1446.60235372
Iteration 923, loss = 1446.56665079
Iteration 924, loss = 1446.65821075
Iteration 925, loss = 1446.52751372
Iteration 926, loss = 1446.49112694
Iteration 927, loss = 1446.49222571
Iteration 928, loss = 1446.42768684
Iteration 929, loss = 1446.46777281
Iteration 930, loss = 1446.45648621
Iteration 931, loss = 1446.43347185
Iteration 932, loss = 1446.43440873
Iteration 933, loss = 1446.30361446
Iteration 934, loss = 1446.31520545
Iteration 935, loss = 1446.26792314
Iteration 936, loss = 1446.20057147
Iteration 937, loss = 1446.24322378
Iteration 938, loss = 1446.18721045
Iteration 939, loss = 1446.43106166
Iteration 940, loss = 1446.17952202
Iteration 941, loss = 1446.11487387
Iteration 942, loss = 1446.00819193
Iteration 943, loss = 1446.02601553
Iteration 944, loss = 1445.98963335
Iteration 945, loss = 1446.01590042
Iteration 946, loss = 1446.08475511
Iteration 947, loss = 1446.40560529
Iteration 948, loss = 1446.02068425
Iteration 949, loss = 1445.93194446
Iteration 950, loss = 1446.31251033
Iteration 951, loss = 1445.80949162
Iteration 952, loss = 1445.80614712
Iteration 953, loss = 1445.81436801
Iteration 954, loss = 1445.73678959
Iteration 955, loss = 1445.71237995
Iteration 956, loss = 1445.87595809
Iteration 957, loss = 1445.96004708
Iteration 958, loss = 1445.76797989
Iteration 959, loss = 1445.68615711
Iteration 960, loss = 1445.67682160
Iteration 961, loss = 1445.53924699
Iteration 962, loss = 1445.52091840
Iteration 963, loss = 1445.48061240
Iteration 964, loss = 1445.67076836
Iteration 965, loss = 1445.54244245
Iteration 966, loss = 1445.54720602
Iteration 967, loss = 1445.54523363
Iteration 968, loss = 1445.38529442
Iteration 969, loss = 1445.34932365
Iteration 970, loss = 1445.31793122
Iteration 971, loss = 1445.36328629
Iteration 972, loss = 1445.30749592
Iteration 973, loss = 1445.32328035
Iteration 974, loss = 1445.35662139
Iteration 975, loss = 1445.19797257
Iteration 976, loss = 1445.15853958
Iteration 977, loss = 1445.28502598
Iteration 978, loss = 1445.13109543
Iteration 979, loss = 1445.17924310
Iteration 980, loss = 1445.08187253
Iteration 981, loss = 1445.02179217
Iteration 982, loss = 1445.51253286
Iteration 983, loss = 1445.22844945
Iteration 984, loss = 1445.03725771
Iteration 985, loss = 1445.00811329
Iteration 986, loss = 1444.91655435
Iteration 987, loss = 1444.94624464
Iteration 988, loss = 1444.97458331
Iteration 989, loss = 1445.03324557
Iteration 990, loss = 1444.81713230
Iteration 991, loss = 1444.78050803
Iteration 992, loss = 1444.71229692
Iteration 993, loss = 1444.71183500
Iteration 994, loss = 1444.76352288
Iteration 995, loss = 1444.80845017
Iteration 996, loss = 1444.78194244
Iteration 997, loss = 1444.85897081
Iteration 998, loss = 1444.93380495
Iteration 999, loss = 1444.84559332
Iteration 1000, loss = 1444.59493156
Iteration 1001, loss = 1444.53972943
Iteration 1002, loss = 1444.53253419
Iteration 1003, loss = 1444.65887694
Iteration 1004, loss = 1444.55849128
Iteration 1005, loss = 1444.78279819
Iteration 1006, loss = 1444.51184962
Iteration 1007, loss = 1444.53042560
Iteration 1008, loss = 1444.50376673
Iteration 1009, loss = 1444.39990108
Iteration 1010, loss = 1444.37870422
Iteration 1011, loss = 1444.38381647
Iteration 1012, loss = 1444.46112563
Iteration 1013, loss = 1444.22018539
Iteration 1014, loss = 1444.18594647
Iteration 1015, loss = 1444.17323666
Iteration 1016, loss = 1444.15454868
Iteration 1017, loss = 1444.15851374
Iteration 1018, loss = 1444.59475217
Iteration 1019, loss = 1444.08369156
Iteration 1020, loss = 1444.01169108
Iteration 1021, loss = 1443.99531395
Iteration 1022, loss = 1444.01394047
Iteration 1023, loss = 1443.98972515
Iteration 1024, loss = 1444.28936663
Iteration 1025, loss = 1443.93903315
Iteration 1026, loss = 1443.93395552
Iteration 1027, loss = 1443.94867052
Iteration 1028, loss = 1443.91728677
Iteration 1029, loss = 1443.86354634
Iteration 1030, loss = 1443.83299829
Iteration 1031, loss = 1443.89307533
Iteration 1032, loss = 1443.75326650
Iteration 1033, loss = 1443.69398005
Iteration 1034, loss = 1443.84087315
Iteration 1035, loss = 1443.70158629
Iteration 1036, loss = 1443.67289943
Iteration 1037, loss = 1443.61109434
Iteration 1038, loss = 1443.57646358
Iteration 1039, loss = 1443.61903569
Iteration 1040, loss = 1443.51858829
Iteration 1041, loss = 1443.64813638
Iteration 1042, loss = 1443.46497264
Iteration 1043, loss = 1443.46031041
Iteration 1044, loss = 1443.40559171
Iteration 1045, loss = 1443.69242350
Iteration 1046, loss = 1443.53724578
Iteration 1047, loss = 1443.36940260
Iteration 1048, loss = 1443.33832777
Iteration 1049, loss = 1443.36952937
Iteration 1050, loss = 1443.29759202
Iteration 1051, loss = 1443.22800743
Iteration 1052, loss = 1443.56869883
Iteration 1053, loss = 1443.23118478
Iteration 1054, loss = 1443.27536321
Iteration 1055, loss = 1443.25043494
Iteration 1056, loss = 1443.12099741
Iteration 1057, loss = 1443.29127095
Iteration 1058, loss = 1443.52718045
Iteration 1059, loss = 1443.16390267
Iteration 1060, loss = 1443.16717433
Iteration 1061, loss = 1443.02028538
Iteration 1062, loss = 1443.03229398
Iteration 1063, loss = 1442.96690479
Iteration 1064, loss = 1443.15854829
Iteration 1065, loss = 1443.05523717
Iteration 1066, loss = 1442.93347926
Iteration 1067, loss = 1442.90622412
Iteration 1068, loss = 1443.04696923
Iteration 1069, loss = 1442.85882599
Iteration 1070, loss = 1442.90327130
Iteration 1071, loss = 1442.89809819
Iteration 1072, loss = 1442.85620812
Iteration 1073, loss = 1442.82799287
Iteration 1074, loss = 1442.75335291
Iteration 1075, loss = 1442.70466838
Iteration 1076, loss = 1442.95683132
Iteration 1077, loss = 1442.62470504
Iteration 1078, loss = 1442.56632661
Iteration 1079, loss = 1442.62825914
Iteration 1080, loss = 1442.50170858
Iteration 1081, loss = 1442.52105444
Iteration 1082, loss = 1442.61626521
Iteration 1083, loss = 1442.72402164
Iteration 1084, loss = 1442.56221557
Iteration 1085, loss = 1442.85450754
Iteration 1086, loss = 1442.57456111
Iteration 1087, loss = 1442.52369718
Iteration 1088, loss = 1442.64950480
Iteration 1089, loss = 1442.52566486
Iteration 1090, loss = 1442.48991524
Iteration 1091, loss = 1442.42230309
Iteration 1092, loss = 1442.32824521
Iteration 1093, loss = 1442.39298490
Iteration 1094, loss = 1442.26811133
Iteration 1095, loss = 1442.18210387
Iteration 1096, loss = 1442.22676211
Iteration 1097, loss = 1442.11326719
Iteration 1098, loss = 1442.12788731
Iteration 1099, loss = 1442.13833003
Iteration 1100, loss = 1442.05819944
Iteration 1101, loss = 1442.29472125
Iteration 1102, loss = 1442.00167482
Iteration 1103, loss = 1441.99996688
Iteration 1104, loss = 1441.95080958
Iteration 1105, loss = 1441.99202053
Iteration 1106, loss = 1441.97951917
Iteration 1107, loss = 1441.86596237
Iteration 1108, loss = 1441.84854367
Iteration 1109, loss = 1441.79872496
Iteration 1110, loss = 1441.80329281
Iteration 1111, loss = 1441.75899315
Iteration 1112, loss = 1441.76258242
Iteration 1113, loss = 1442.18464110
Iteration 1114, loss = 1441.82704071
Iteration 1115, loss = 1441.76643485
Iteration 1116, loss = 1441.66643315
Iteration 1117, loss = 1441.59545047
Iteration 1118, loss = 1441.96282934
Iteration 1119, loss = 1441.63571624
Iteration 1120, loss = 1441.62692098
Iteration 1121, loss = 1441.56408012
Iteration 1122, loss = 1441.53437502
Iteration 1123, loss = 1442.22216774
Iteration 1124, loss = 1441.68236696
Iteration 1125, loss = 1441.73703650
Iteration 1126, loss = 1441.42220831
Iteration 1127, loss = 1441.48827249
Iteration 1128, loss = 1441.58343171
Iteration 1129, loss = 1441.49353504
Iteration 1130, loss = 1441.33705926
Iteration 1131, loss = 1441.33146928
Iteration 1132, loss = 1441.29499225
Iteration 1133, loss = 1441.28765627
Iteration 1134, loss = 1441.27705451
Iteration 1135, loss = 1441.42128216
Iteration 1136, loss = 1441.99799652
Iteration 1137, loss = 1441.18880890
Iteration 1138, loss = 1441.26887001
Iteration 1139, loss = 1441.18808131
Iteration 1140, loss = 1441.10756999
Iteration 1141, loss = 1441.22314293
Iteration 1142, loss = 1441.22886724
Iteration 1143, loss = 1441.21900071
Iteration 1144, loss = 1441.14208714
Iteration 1145, loss = 1441.07944939
Iteration 1146, loss = 1441.00247851
Iteration 1147, loss = 1441.12017662
Iteration 1148, loss = 1440.97331460
Iteration 1149, loss = 1440.93800381
Iteration 1150, loss = 1440.95346124
Iteration 1151, loss = 1441.17135768
Iteration 1152, loss = 1440.98839997
Iteration 1153, loss = 1440.92017238
Iteration 1154, loss = 1441.16751531
Iteration 1155, loss = 1440.97017644
Iteration 1156, loss = 1440.78236337
Iteration 1157, loss = 1440.82025463
Iteration 1158, loss = 1441.27554853
Iteration 1159, loss = 1440.85216300
Iteration 1160, loss = 1440.77072151
Iteration 1161, loss = 1440.73316290
Iteration 1162, loss = 1440.77559492
Iteration 1163, loss = 1440.66316135
Iteration 1164, loss = 1440.70308148
Iteration 1165, loss = 1440.67735718
Iteration 1166, loss = 1440.58706758
Iteration 1167, loss = 1440.77517093
Iteration 1168, loss = 1440.53175703
Iteration 1169, loss = 1440.63723015
Iteration 1170, loss = 1440.56881302
Iteration 1171, loss = 1440.52292880
Iteration 1172, loss = 1440.49977278
Iteration 1173, loss = 1440.48562507
Iteration 1174, loss = 1440.57049310
Iteration 1175, loss = 1440.89914436
Iteration 1176, loss = 1440.36740120
Iteration 1177, loss = 1440.35986343
Iteration 1178, loss = 1440.41489114
Iteration 1179, loss = 1440.31987374
Iteration 1180, loss = 1440.30495529
Iteration 1181, loss = 1440.32879118
Iteration 1182, loss = 1440.40497478
Iteration 1183, loss = 1440.28273858
Iteration 1184, loss = 1440.23601385
Iteration 1185, loss = 1440.20084001
Iteration 1186, loss = 1440.16383450
Iteration 1187, loss = 1440.16016171
Iteration 1188, loss = 1440.10964702
Iteration 1189, loss = 1440.13577087
Iteration 1190, loss = 1440.22226220
Iteration 1191, loss = 1440.08453862
Iteration 1192, loss = 1440.07671973
Iteration 1193, loss = 1440.20694230
Iteration 1194, loss = 1440.07561833
Iteration 1195, loss = 1439.96061434
Iteration 1196, loss = 1439.94675741
Iteration 1197, loss = 1439.93954325
Iteration 1198, loss = 1439.92185013
Iteration 1199, loss = 1439.85941324
Iteration 1200, loss = 1439.85059202
Iteration 1201, loss = 1439.99062788
Iteration 1202, loss = 1439.87034670
Iteration 1203, loss = 1439.78231140
Iteration 1204, loss = 1439.77305004
Iteration 1205, loss = 1439.76761061
Iteration 1206, loss = 1439.71134747
Iteration 1207, loss = 1439.96893469
Iteration 1208, loss = 1439.70541936
Iteration 1209, loss = 1439.64887689
Iteration 1210, loss = 1439.64886506
Iteration 1211, loss = 1439.67651588
Iteration 1212, loss = 1439.62046494
Iteration 1213, loss = 1439.60041063
Iteration 1214, loss = 1439.64040151
Iteration 1215, loss = 1439.49708289
Iteration 1216, loss = 1439.74122650
Iteration 1217, loss = 1439.54970003
Iteration 1218, loss = 1439.48692342
Iteration 1219, loss = 1439.46390264
Iteration 1220, loss = 1439.47118023
Iteration 1221, loss = 1439.41976211
Iteration 1222, loss = 1439.50631188
Iteration 1223, loss = 1439.31723086
Iteration 1224, loss = 1439.37070938
Iteration 1225, loss = 1439.30816497
Iteration 1226, loss = 1439.31974614
Iteration 1227, loss = 1439.48839071
Iteration 1228, loss = 1439.35462026
Iteration 1229, loss = 1439.57336494
Iteration 1230, loss = 1439.21544667
Iteration 1231, loss = 1439.16463252
Iteration 1232, loss = 1439.20205107
Iteration 1233, loss = 1439.14012868
Iteration 1234, loss = 1439.09839095
Iteration 1235, loss = 1439.08318285
Iteration 1236, loss = 1439.09319636
Iteration 1237, loss = 1439.04647609
Iteration 1238, loss = 1439.18849508
Iteration 1239, loss = 1439.00014524
Iteration 1240, loss = 1438.97150307
Iteration 1241, loss = 1438.95134589
Iteration 1242, loss = 1439.01195468
Iteration 1243, loss = 1438.90342193
Iteration 1244, loss = 1439.00980627
Iteration 1245, loss = 1438.90394654
Iteration 1246, loss = 1439.04190874
Iteration 1247, loss = 1438.80762844
Iteration 1248, loss = 1438.91867388
Iteration 1249, loss = 1438.84048419
Iteration 1250, loss = 1438.77357753
Iteration 1251, loss = 1438.76685993
Iteration 1252, loss = 1438.81199832
Iteration 1253, loss = 1438.76991815
Iteration 1254, loss = 1438.93797280
Iteration 1255, loss = 1438.68118202
Iteration 1256, loss = 1438.72611661
Iteration 1257, loss = 1438.63375470
Iteration 1258, loss = 1438.60789653
Iteration 1259, loss = 1438.60895629
Iteration 1260, loss = 1438.67310556
Iteration 1261, loss = 1438.56737716
Iteration 1262, loss = 1438.60556810
Iteration 1263, loss = 1438.61488426
Iteration 1264, loss = 1438.51765526
Iteration 1265, loss = 1438.71867327
Iteration 1266, loss = 1438.54494105
Iteration 1267, loss = 1438.82736863
Iteration 1268, loss = 1438.44382280
Iteration 1269, loss = 1438.42826223
Iteration 1270, loss = 1438.43542430
Iteration 1271, loss = 1438.46903202
Iteration 1272, loss = 1438.40549701
Iteration 1273, loss = 1438.39027765
Iteration 1274, loss = 1438.29904071
Iteration 1275, loss = 1438.28919363
Iteration 1276, loss = 1438.21612108
Iteration 1277, loss = 1438.37721533
Iteration 1278, loss = 1438.19273494
Iteration 1279, loss = 1438.36950091
Iteration 1280, loss = 1438.30635010
Iteration 1281, loss = 1438.35189006
Iteration 1282, loss = 1438.21074871
Iteration 1283, loss = 1438.34062836
Iteration 1284, loss = 1438.11482316
Iteration 1285, loss = 1438.08808420
Iteration 1286, loss = 1438.14696220
Iteration 1287, loss = 1438.03017409
Iteration 1288, loss = 1438.19429601
Iteration 1289, loss = 1438.07271997
Iteration 1290, loss = 1438.15418159
Iteration 1291, loss = 1438.01570099
Iteration 1292, loss = 1438.01396388
Iteration 1293, loss = 1438.03831024
Iteration 1294, loss = 1438.10232210
Iteration 1295, loss = 1437.88997505
Iteration 1296, loss = 1437.86549844
Iteration 1297, loss = 1437.89731410
Iteration 1298, loss = 1437.81088293
Iteration 1299, loss = 1437.79738318
Iteration 1300, loss = 1437.95266171
Iteration 1301, loss = 1437.90603008
Iteration 1302, loss = 1437.75676568
Iteration 1303, loss = 1437.75986851
Iteration 1304, loss = 1437.74401243
Iteration 1305, loss = 1437.93609590
Iteration 1306, loss = 1437.79819130
Iteration 1307, loss = 1437.73052520
Iteration 1308, loss = 1438.03945091
Iteration 1309, loss = 1437.65166423
Iteration 1310, loss = 1437.62995853
Iteration 1311, loss = 1437.62054464
Iteration 1312, loss = 1437.63048026
Iteration 1313, loss = 1437.58360102
Iteration 1314, loss = 1437.58630287
Iteration 1315, loss = 1437.47235206
Iteration 1316, loss = 1437.43121423
Iteration 1317, loss = 1437.48685965
Iteration 1318, loss = 1437.38083948
Iteration 1319, loss = 1437.41002080
Iteration 1320, loss = 1437.38176788
Iteration 1321, loss = 1437.47582033
Iteration 1322, loss = 1437.37401534
Iteration 1323, loss = 1437.35053493
Iteration 1324, loss = 1437.56092610
Iteration 1325, loss = 1437.34341081
Iteration 1326, loss = 1437.24771540
Iteration 1327, loss = 1437.20583769
Iteration 1328, loss = 1437.23920811
Iteration 1329, loss = 1437.23373633
Iteration 1330, loss = 1437.38662238
Iteration 1331, loss = 1437.11771011
Iteration 1332, loss = 1437.29219949
Iteration 1333, loss = 1437.17592896
Iteration 1334, loss = 1437.22494345
Iteration 1335, loss = 1437.10812469
Iteration 1336, loss = 1437.04279350
Iteration 1337, loss = 1437.06196908
Iteration 1338, loss = 1437.02002831
Iteration 1339, loss = 1436.98420531
Iteration 1340, loss = 1436.94008855
Iteration 1341, loss = 1437.04567890
Iteration 1342, loss = 1436.93793748
Iteration 1343, loss = 1436.86045207
Iteration 1344, loss = 1436.82695458
Iteration 1345, loss = 1436.88739307
Iteration 1346, loss = 1436.81074628
Iteration 1347, loss = 1436.86001142
Iteration 1348, loss = 1436.76139300
Iteration 1349, loss = 1436.74281812
Iteration 1350, loss = 1436.88674939
Iteration 1351, loss = 1436.70592816
Iteration 1352, loss = 1436.88883681
Iteration 1353, loss = 1437.45560017
Iteration 1354, loss = 1436.67846614
Iteration 1355, loss = 1436.73972494
Iteration 1356, loss = 1436.57621547
Iteration 1357, loss = 1436.55212698
Iteration 1358, loss = 1436.51429126
Iteration 1359, loss = 1436.54536548
Iteration 1360, loss = 1436.57178254
Iteration 1361, loss = 1436.57482547
Iteration 1362, loss = 1436.50427343
Iteration 1363, loss = 1436.45433821
Iteration 1364, loss = 1436.48209580
Iteration 1365, loss = 1436.47275138
Iteration 1366, loss = 1436.46171751
Iteration 1367, loss = 1436.54936983
Iteration 1368, loss = 1436.49547935
Iteration 1369, loss = 1436.60046326
Iteration 1370, loss = 1436.35690563
Iteration 1371, loss = 1436.29157841
Iteration 1372, loss = 1436.32843558
Iteration 1373, loss = 1436.29566426
Iteration 1374, loss = 1436.22005129
Iteration 1375, loss = 1436.23594500
Iteration 1376, loss = 1436.27882488
Iteration 1377, loss = 1436.15930790
Iteration 1378, loss = 1436.14520625
Iteration 1379, loss = 1436.12386656
Iteration 1380, loss = 1436.12754484
Iteration 1381, loss = 1436.09725495
Iteration 1382, loss = 1436.11263554
Iteration 1383, loss = 1436.27606237
Iteration 1384, loss = 1436.03476048
Iteration 1385, loss = 1436.01212274
Iteration 1386, loss = 1436.05327054
Iteration 1387, loss = 1435.91366408
Iteration 1388, loss = 1436.09471483
Iteration 1389, loss = 1435.89051238
Iteration 1390, loss = 1436.16274989
Iteration 1391, loss = 1435.81712021
Iteration 1392, loss = 1435.83693760
Iteration 1393, loss = 1435.86574523
Iteration 1394, loss = 1435.77319131
Iteration 1395, loss = 1435.73721185
Iteration 1396, loss = 1435.75426952
Iteration 1397, loss = 1435.72094297
Iteration 1398, loss = 1435.71665142
Iteration 1399, loss = 1435.68039699
Iteration 1400, loss = 1435.68434112
Iteration 1401, loss = 1435.59675181
Iteration 1402, loss = 1435.58339949
Iteration 1403, loss = 1435.92868114
Iteration 1404, loss = 1435.53807000
Iteration 1405, loss = 1435.63446323
Iteration 1406, loss = 1435.52132752
Iteration 1407, loss = 1435.50855674
Iteration 1408, loss = 1435.92146273
Iteration 1409, loss = 1435.48475457
Iteration 1410, loss = 1435.57511152
Iteration 1411, loss = 1435.45977555
Iteration 1412, loss = 1435.49022009
Iteration 1413, loss = 1435.55543506
Iteration 1414, loss = 1435.34597661
Iteration 1415, loss = 1435.25788928
Iteration 1416, loss = 1435.25176104
Iteration 1417, loss = 1435.22863403
Iteration 1418, loss = 1435.47741902
Iteration 1419, loss = 1435.18870316
Iteration 1420, loss = 1435.15885053
Iteration 1421, loss = 1435.18756466
Iteration 1422, loss = 1435.12496390
Iteration 1423, loss = 1435.42307186
Iteration 1424, loss = 1435.10053753
Iteration 1425, loss = 1435.06775761
Iteration 1426, loss = 1435.34801957
Iteration 1427, loss = 1435.05575420
Iteration 1428, loss = 1435.07863704
Iteration 1429, loss = 1435.13590771
Iteration 1430, loss = 1435.00868815
Iteration 1431, loss = 1434.87453567
Iteration 1432, loss = 1434.85115156
Iteration 1433, loss = 1434.86030079
Iteration 1434, loss = 1434.85381408
Iteration 1435, loss = 1434.82090289
Iteration 1436, loss = 1434.85677177
Iteration 1437, loss = 1434.73558080
Iteration 1438, loss = 1434.70880692
Iteration 1439, loss = 1434.67515611
Iteration 1440, loss = 1434.71429952
Iteration 1441, loss = 1434.75236262
Iteration 1442, loss = 1434.61000855
Iteration 1443, loss = 1434.71157380
Iteration 1444, loss = 1434.83146297
Iteration 1445, loss = 1434.56911618
Iteration 1446, loss = 1434.51298093
Iteration 1447, loss = 1434.56756406
Iteration 1448, loss = 1434.60376269
Iteration 1449, loss = 1434.48237062
Iteration 1450, loss = 1434.37747930
Iteration 1451, loss = 1434.39981335
Iteration 1452, loss = 1434.42792654
Iteration 1453, loss = 1434.44093298
Iteration 1454, loss = 1434.41947316
Iteration 1455, loss = 1434.37837782
Iteration 1456, loss = 1434.38237181
Iteration 1457, loss = 1434.20549576
Iteration 1458, loss = 1434.20928820
Iteration 1459, loss = 1434.46603401
Iteration 1460, loss = 1434.35344285
Iteration 1461, loss = 1434.77820608
Iteration 1462, loss = 1434.26391191
Iteration 1463, loss = 1434.29868341
Iteration 1464, loss = 1434.30738168
Iteration 1465, loss = 1434.06418663
Iteration 1466, loss = 1434.06336935
Iteration 1467, loss = 1434.36151120
Iteration 1468, loss = 1434.00556977
Iteration 1469, loss = 1434.06458639
Iteration 1470, loss = 1434.23713946
Iteration 1471, loss = 1433.92607384
Iteration 1472, loss = 1433.91780135
Iteration 1473, loss = 1433.85718360
Iteration 1474, loss = 1433.85052096
Iteration 1475, loss = 1433.82323180
Iteration 1476, loss = 1433.77693046
Iteration 1477, loss = 1433.69878726
Iteration 1478, loss = 1434.13958660
Iteration 1479, loss = 1434.13464867
Iteration 1480, loss = 1433.89177980
Iteration 1481, loss = 1433.67344298
Iteration 1482, loss = 1433.62173707
Iteration 1483, loss = 1433.57986873
Iteration 1484, loss = 1433.50826751
Iteration 1485, loss = 1433.66549874
Iteration 1486, loss = 1433.44925260
Iteration 1487, loss = 1433.55122041
Iteration 1488, loss = 1433.54937297
Iteration 1489, loss = 1433.38300527
Iteration 1490, loss = 1433.54839904
Iteration 1491, loss = 1433.31202558
Iteration 1492, loss = 1433.27525747
Iteration 1493, loss = 1433.16715878
Iteration 1494, loss = 1433.14259333
Iteration 1495, loss = 1433.21808542
Iteration 1496, loss = 1433.07995422
Iteration 1497, loss = 1433.05453153
Iteration 1498, loss = 1433.12746471
Iteration 1499, loss = 1433.26113103
Iteration 1500, loss = 1432.93633340
Iteration 1501, loss = 1433.17055722
Iteration 1502, loss = 1433.09375542
Iteration 1503, loss = 1432.80160644
Iteration 1504, loss = 1432.76016352
Iteration 1505, loss = 1432.76668136
Iteration 1506, loss = 1432.82403840
Iteration 1507, loss = 1432.75871539
Iteration 1508, loss = 1432.73665326
Iteration 1509, loss = 1432.77160022
Iteration 1510, loss = 1432.77099409
Iteration 1511, loss = 1432.70599248
Iteration 1512, loss = 1432.63164506
Iteration 1513, loss = 1432.73775752
Iteration 1514, loss = 1432.95562842
Iteration 1515, loss = 1432.48526151
Iteration 1516, loss = 1432.33337530
Iteration 1517, loss = 1432.33915861
Iteration 1518, loss = 1432.27866290
Iteration 1519, loss = 1432.41900582
Iteration 1520, loss = 1432.57781588
Iteration 1521, loss = 1432.33994182
Iteration 1522, loss = 1432.08142845
Iteration 1523, loss = 1432.67273804
Iteration 1524, loss = 1432.00259448
Iteration 1525, loss = 1431.99114259
Iteration 1526, loss = 1432.01385166
Iteration 1527, loss = 1431.91642817
Iteration 1528, loss = 1431.91503230
Iteration 1529, loss = 1431.85856199
Iteration 1530, loss = 1431.77205669
Iteration 1531, loss = 1432.22120207
Iteration 1532, loss = 1431.91428165
Iteration 1533, loss = 1431.62971720
Iteration 1534, loss = 1431.82780917
Iteration 1535, loss = 1431.72173005
Iteration 1536, loss = 1431.72717823
Iteration 1537, loss = 1431.58477164
Iteration 1538, loss = 1431.54398093
Iteration 1539, loss = 1431.51040287
Iteration 1540, loss = 1431.32663290
Iteration 1541, loss = 1431.43092940
Iteration 1542, loss = 1431.21219859
Iteration 1543, loss = 1431.27442141
Iteration 1544, loss = 1431.11879224
Iteration 1545, loss = 1431.13673268
Iteration 1546, loss = 1431.03706570
Iteration 1547, loss = 1431.09593974
Iteration 1548, loss = 1431.32641978
Iteration 1549, loss = 1431.14511482
Iteration 1550, loss = 1430.89630818
Iteration 1551, loss = 1430.93612222
Iteration 1552, loss = 1431.21248612
Iteration 1553, loss = 1430.75697948
Iteration 1554, loss = 1430.70101040
Iteration 1555, loss = 1430.67895295
Iteration 1556, loss = 1430.78741327
Iteration 1557, loss = 1430.57800846
Iteration 1558, loss = 1430.62159757
Iteration 1559, loss = 1430.67698827
Iteration 1560, loss = 1430.58306887
Iteration 1561, loss = 1430.57011138
Iteration 1562, loss = 1430.59151606
Iteration 1563, loss = 1430.64577329
Iteration 1564, loss = 1430.54334505
Iteration 1565, loss = 1430.26821836
Iteration 1566, loss = 1430.23337258
Iteration 1567, loss = 1430.51290271
Iteration 1568, loss = 1430.57541004
Iteration 1569, loss = 1430.16193731
Iteration 1570, loss = 1430.14280997
Iteration 1571, loss = 1430.08496628
Iteration 1572, loss = 1430.28787872
Iteration 1573, loss = 1430.06243853
Iteration 1574, loss = 1429.96625951
Iteration 1575, loss = 1430.28340294
Iteration 1576, loss = 1430.03378752
Iteration 1577, loss = 1429.92696919
Iteration 1578, loss = 1429.82823748
Iteration 1579, loss = 1429.81574140
Iteration 1580, loss = 1430.42895407
Iteration 1581, loss = 1429.91819682
Iteration 1582, loss = 1430.04160060
Iteration 1583, loss = 1429.63595184
Iteration 1584, loss = 1429.78221661
Iteration 1585, loss = 1429.67399950
Iteration 1586, loss = 1429.68105403
Iteration 1587, loss = 1429.59749499
Iteration 1588, loss = 1429.79409680
Iteration 1589, loss = 1429.46985935
Iteration 1590, loss = 1429.52706934
Iteration 1591, loss = 1429.38653091
Iteration 1592, loss = 1429.70194689
Iteration 1593, loss = 1429.33472209
Iteration 1594, loss = 1429.90859640
Iteration 1595, loss = 1429.35763358
Iteration 1596, loss = 1429.24506696
Iteration 1597, loss = 1429.42081086
Iteration 1598, loss = 1429.09156724
Iteration 1599, loss = 1429.04305049
Iteration 1600, loss = 1429.03257719
Iteration 1601, loss = 1429.03545937
Iteration 1602, loss = 1429.34129258
Iteration 1603, loss = 1428.82507322
Iteration 1604, loss = 1428.85436817
Iteration 1605, loss = 1428.78458106
Iteration 1606, loss = 1428.82402761
Iteration 1607, loss = 1428.72034505
Iteration 1608, loss = 1428.70185827
Iteration 1609, loss = 1428.77220582
Iteration 1610, loss = 1428.59861744
Iteration 1611, loss = 1428.51062523
Iteration 1612, loss = 1428.46684747
Iteration 1613, loss = 1428.36980404
Iteration 1614, loss = 1428.34307747
Iteration 1615, loss = 1428.28550179
Iteration 1616, loss = 1428.25468472
Iteration 1617, loss = 1428.28971487
Iteration 1618, loss = 1428.21671657
Iteration 1619, loss = 1428.30736284
Iteration 1620, loss = 1428.14937453
Iteration 1621, loss = 1428.01931563
Iteration 1622, loss = 1428.15747045
Iteration 1623, loss = 1427.86101182
Iteration 1624, loss = 1428.04286280
Iteration 1625, loss = 1427.71286404
Iteration 1626, loss = 1427.93763054
Iteration 1627, loss = 1427.68830373
Iteration 1628, loss = 1427.77822756
Iteration 1629, loss = 1427.70305017
Iteration 1630, loss = 1427.84121155
Iteration 1631, loss = 1427.53963370
Iteration 1632, loss = 1427.42443805
Iteration 1633, loss = 1427.36922859
Iteration 1634, loss = 1427.34336013
Iteration 1635, loss = 1427.20404654
Iteration 1636, loss = 1427.15433416
Iteration 1637, loss = 1427.41867481
Iteration 1638, loss = 1426.96337261
Iteration 1639, loss = 1426.88064115
Iteration 1640, loss = 1426.91476819
Iteration 1641, loss = 1426.80077169
Iteration 1642, loss = 1426.67126075
Iteration 1643, loss = 1426.58646833
Iteration 1644, loss = 1426.96229666
Iteration 1645, loss = 1426.54776028
Iteration 1646, loss = 1426.42439846
Iteration 1647, loss = 1426.35094720
Iteration 1648, loss = 1426.32088116
Iteration 1649, loss = 1426.29875539
Iteration 1650, loss = 1426.38180168
Iteration 1651, loss = 1426.12175801
Iteration 1652, loss = 1426.04323405
Iteration 1653, loss = 1426.04363379
Iteration 1654, loss = 1426.00849764
Iteration 1655, loss = 1426.04790311
Iteration 1656, loss = 1426.07061450
Iteration 1657, loss = 1426.28974622
Iteration 1658, loss = 1425.78298844
Iteration 1659, loss = 1425.67702172
Iteration 1660, loss = 1425.63513825
Iteration 1661, loss = 1425.62257980
Iteration 1662, loss = 1425.50791400
Iteration 1663, loss = 1425.46727444
Iteration 1664, loss = 1425.39411490
Iteration 1665, loss = 1425.41580400
Iteration 1666, loss = 1425.37450037
Iteration 1667, loss = 1425.66405913
Iteration 1668, loss = 1425.19311965
Iteration 1669, loss = 1425.22551182
Iteration 1670, loss = 1425.22561440
Iteration 1671, loss = 1425.00153313
Iteration 1672, loss = 1424.94221422
Iteration 1673, loss = 1424.84728099
Iteration 1674, loss = 1424.84123501
Iteration 1675, loss = 1424.73729473
Iteration 1676, loss = 1424.64551194
Iteration 1677, loss = 1424.78896628
Iteration 1678, loss = 1424.57847805
Iteration 1679, loss = 1424.92016370
Iteration 1680, loss = 1424.44245757
Iteration 1681, loss = 1424.40779407
Iteration 1682, loss = 1424.29981592
Iteration 1683, loss = 1424.26985571
Iteration 1684, loss = 1424.18789458
Iteration 1685, loss = 1424.25693527
Iteration 1686, loss = 1424.23815737
Iteration 1687, loss = 1424.14219764
Iteration 1688, loss = 1424.04676543
Iteration 1689, loss = 1423.99652759
Iteration 1690, loss = 1423.98808886
Iteration 1691, loss = 1423.82742499
Iteration 1692, loss = 1423.90515636
Iteration 1693, loss = 1423.67177127
Iteration 1694, loss = 1423.59322990
Iteration 1695, loss = 1423.56149116
Iteration 1696, loss = 1423.56070463
Iteration 1697, loss = 1423.46521512
Iteration 1698, loss = 1423.38640291
Iteration 1699, loss = 1423.27483957
Iteration 1700, loss = 1423.16941058
Iteration 1701, loss = 1423.10227880
Iteration 1702, loss = 1423.41303290
Iteration 1703, loss = 1422.97237342
Iteration 1704, loss = 1422.86437627
Iteration 1705, loss = 1422.84442329
Iteration 1706, loss = 1422.69750377
Iteration 1707, loss = 1423.48356943
Iteration 1708, loss = 1422.54493363
Iteration 1709, loss = 1422.45833098
Iteration 1710, loss = 1422.45147644
Iteration 1711, loss = 1422.46439845
Iteration 1712, loss = 1422.49587427
Iteration 1713, loss = 1422.52811108
Iteration 1714, loss = 1422.36716543
Iteration 1715, loss = 1422.17580365
Iteration 1716, loss = 1422.08657273
Iteration 1717, loss = 1421.92086132
Iteration 1718, loss = 1421.82905791
Iteration 1719, loss = 1421.86318115
Iteration 1720, loss = 1421.80474239
Iteration 1721, loss = 1421.73255794
Iteration 1722, loss = 1421.73498791
Iteration 1723, loss = 1421.70760544
Iteration 1724, loss = 1421.69609835
Iteration 1725, loss = 1421.78618332
Iteration 1726, loss = 1421.43562645
Iteration 1727, loss = 1421.63117321
Iteration 1728, loss = 1421.28063468
Iteration 1729, loss = 1421.26764870
Iteration 1730, loss = 1421.25588063
Iteration 1731, loss = 1421.40171882
Iteration 1732, loss = 1421.04799317
Iteration 1733, loss = 1420.98957241
Iteration 1734, loss = 1421.03524877
Iteration 1735, loss = 1420.99529647
Iteration 1736, loss = 1421.57980281
Iteration 1737, loss = 1420.89236685
Iteration 1738, loss = 1420.78072505
Iteration 1739, loss = 1420.70166129
Iteration 1740, loss = 1420.88014258
Iteration 1741, loss = 1420.58244169
Iteration 1742, loss = 1420.87912261
Iteration 1743, loss = 1420.66320324
Iteration 1744, loss = 1420.36431757
Iteration 1745, loss = 1420.35848193
Iteration 1746, loss = 1420.27039561
Iteration 1747, loss = 1420.89344911
Iteration 1748, loss = 1420.33161882
Iteration 1749, loss = 1420.21461744
Iteration 1750, loss = 1420.26834134
Iteration 1751, loss = 1420.07623724
Iteration 1752, loss = 1419.99800761
Iteration 1753, loss = 1420.14765127
Iteration 1754, loss = 1419.98537837
Iteration 1755, loss = 1419.78882762
Iteration 1756, loss = 1419.75997979
Iteration 1757, loss = 1420.15274169
Iteration 1758, loss = 1419.73336290
Iteration 1759, loss = 1419.65706657
Iteration 1760, loss = 1419.54723730
Iteration 1761, loss = 1419.59095164
Iteration 1762, loss = 1419.61932059
Iteration 1763, loss = 1419.45268448
Iteration 1764, loss = 1419.32292247
Iteration 1765, loss = 1420.02132663
Iteration 1766, loss = 1419.61381580
Iteration 1767, loss = 1419.65691830
Iteration 1768, loss = 1419.25458080
Iteration 1769, loss = 1419.13881702
Iteration 1770, loss = 1419.14388899
Iteration 1771, loss = 1419.16286120
Iteration 1772, loss = 1419.12897589
Iteration 1773, loss = 1418.89755426
Iteration 1774, loss = 1419.16309615
Iteration 1775, loss = 1418.81948563
Iteration 1776, loss = 1418.83554749
Iteration 1777, loss = 1418.76198996
Iteration 1778, loss = 1418.94280250
Iteration 1779, loss = 1418.63876527
Iteration 1780, loss = 1418.83261841
Iteration 1781, loss = 1418.52568128
Iteration 1782, loss = 1418.45903780
Iteration 1783, loss = 1418.43509927
Iteration 1784, loss = 1418.57331486
Iteration 1785, loss = 1418.34246007
Iteration 1786, loss = 1418.25951442
Iteration 1787, loss = 1418.25532287
Iteration 1788, loss = 1418.30576503
Iteration 1789, loss = 1418.49037516
Iteration 1790, loss = 1418.14368342
Iteration 1791, loss = 1418.06479627
Iteration 1792, loss = 1418.02482557
Iteration 1793, loss = 1417.93969221
Iteration 1794, loss = 1417.87619930
Iteration 1795, loss = 1417.83495237
Iteration 1796, loss = 1417.81549591
Iteration 1797, loss = 1417.98129101
Iteration 1798, loss = 1417.69836375
Iteration 1799, loss = 1417.71792844
Iteration 1800, loss = 1417.72749540
Iteration 1801, loss = 1417.78983093
Iteration 1802, loss = 1417.45114723
Iteration 1803, loss = 1417.49047767
Iteration 1804, loss = 1417.43736861
Iteration 1805, loss = 1417.61937491
Iteration 1806, loss = 1417.23939470
Iteration 1807, loss = 1417.23793028
Iteration 1808, loss = 1417.11297009
Iteration 1809, loss = 1417.08076160
Iteration 1810, loss = 1417.13211099
Iteration 1811, loss = 1417.14971697
Iteration 1812, loss = 1417.01757533
Iteration 1813, loss = 1417.02906326
Iteration 1814, loss = 1416.81597689
Iteration 1815, loss = 1417.06247014
Iteration 1816, loss = 1416.77396753
Iteration 1817, loss = 1416.66815721
Iteration 1818, loss = 1416.67594047
Iteration 1819, loss = 1416.61695654
Iteration 1820, loss = 1416.52170546
Iteration 1821, loss = 1416.64346330
Iteration 1822, loss = 1416.46455304
Iteration 1823, loss = 1416.53249408
Iteration 1824, loss = 1416.34521564
Iteration 1825, loss = 1416.23882815
Iteration 1826, loss = 1416.35446654
Iteration 1827, loss = 1416.28697340
Iteration 1828, loss = 1416.07197313
Iteration 1829, loss = 1416.54056199
Iteration 1830, loss = 1416.03751324
Iteration 1831, loss = 1415.99355248
Iteration 1832, loss = 1415.94526604
Iteration 1833, loss = 1415.83088506
Iteration 1834, loss = 1415.77783533
Iteration 1835, loss = 1415.74428823
Iteration 1836, loss = 1415.67138857
Iteration 1837, loss = 1415.73575046
Iteration 1838, loss = 1415.60684956
Iteration 1839, loss = 1415.54811434
Iteration 1840, loss = 1415.85259869
Iteration 1841, loss = 1415.59801022
Iteration 1842, loss = 1415.40810280
Iteration 1843, loss = 1415.43218051
Iteration 1844, loss = 1415.28707524
Iteration 1845, loss = 1415.26235771
Iteration 1846, loss = 1415.20931653
Iteration 1847, loss = 1415.21911582
Iteration 1848, loss = 1415.08901220
Iteration 1849, loss = 1414.99410428
Iteration 1850, loss = 1415.22397866
Iteration 1851, loss = 1415.13966302
Iteration 1852, loss = 1414.97743759
Iteration 1853, loss = 1414.83793414
Iteration 1854, loss = 1414.78661332
Iteration 1855, loss = 1414.99853516
Iteration 1856, loss = 1414.86188684
Iteration 1857, loss = 1415.42390875
Iteration 1858, loss = 1414.63858441
Iteration 1859, loss = 1414.63413226
Iteration 1860, loss = 1414.50289257
Iteration 1861, loss = 1414.53390975
Iteration 1862, loss = 1414.45216505
Iteration 1863, loss = 1414.53809348
Iteration 1864, loss = 1414.32961027
Iteration 1865, loss = 1414.34558841
Iteration 1866, loss = 1414.44262557
Iteration 1867, loss = 1414.17154568
Iteration 1868, loss = 1414.12007988
Iteration 1869, loss = 1414.05855050
Iteration 1870, loss = 1413.95443472
Iteration 1871, loss = 1413.84383808
Iteration 1872, loss = 1414.05455829
Iteration 1873, loss = 1413.84579434
Iteration 1874, loss = 1413.82576633
Iteration 1875, loss = 1413.81626761
Iteration 1876, loss = 1414.08389337
Iteration 1877, loss = 1413.60515103
Iteration 1878, loss = 1413.86765789
Iteration 1879, loss = 1413.53972015
Iteration 1880, loss = 1413.54182408
Iteration 1881, loss = 1413.60889682
Iteration 1882, loss = 1413.47199037
Iteration 1883, loss = 1413.47861202
Iteration 1884, loss = 1413.28859088
Iteration 1885, loss = 1413.19497159
Iteration 1886, loss = 1413.23210655
Iteration 1887, loss = 1413.57620818
Iteration 1888, loss = 1413.04037973
Iteration 1889, loss = 1413.01674436
Iteration 1890, loss = 1412.93632485
Iteration 1891, loss = 1412.89014112
Iteration 1892, loss = 1412.84798620
Iteration 1893, loss = 1412.81552038
Iteration 1894, loss = 1412.81245722
Iteration 1895, loss = 1412.70740537
Iteration 1896, loss = 1412.63695964
Iteration 1897, loss = 1412.57991607
Iteration 1898, loss = 1412.52849655
Iteration 1899, loss = 1412.53242810
Iteration 1900, loss = 1412.42948721
Iteration 1901, loss = 1412.60275605
Iteration 1902, loss = 1412.31464815
Iteration 1903, loss = 1412.31993620
Iteration 1904, loss = 1412.33207335
Iteration 1905, loss = 1412.52001373
Iteration 1906, loss = 1412.58133604
Iteration 1907, loss = 1412.27041602
Iteration 1908, loss = 1412.31093174
Iteration 1909, loss = 1412.18254930
Iteration 1910, loss = 1412.50686848
Iteration 1911, loss = 1412.01352225
Iteration 1912, loss = 1411.92591971
Iteration 1913, loss = 1411.84616138
Iteration 1914, loss = 1411.87176656
Iteration 1915, loss = 1411.96906093
Iteration 1916, loss = 1411.85637685
Iteration 1917, loss = 1412.07238678
Iteration 1918, loss = 1411.69106705
Iteration 1919, loss = 1411.58637091
Iteration 1920, loss = 1411.48908938
Iteration 1921, loss = 1411.35207526
Iteration 1922, loss = 1411.28357871
Iteration 1923, loss = 1411.26824964
Iteration 1924, loss = 1411.63257066
Iteration 1925, loss = 1411.27247242
Iteration 1926, loss = 1411.24900887
Iteration 1927, loss = 1411.23245711
Iteration 1928, loss = 1411.09198643
Iteration 1929, loss = 1411.37122218
Iteration 1930, loss = 1410.81925519
Iteration 1931, loss = 1411.05623567
Iteration 1932, loss = 1410.91788892
Iteration 1933, loss = 1410.89736139
Iteration 1934, loss = 1410.86031644
Iteration 1935, loss = 1410.75195438
Iteration 1936, loss = 1410.66547393
Iteration 1937, loss = 1410.61175471
Iteration 1938, loss = 1410.46873329
Iteration 1939, loss = 1410.39302732
Iteration 1940, loss = 1410.44748552
Iteration 1941, loss = 1410.32676058
Iteration 1942, loss = 1410.38647233
Iteration 1943, loss = 1411.02367334
Iteration 1944, loss = 1410.37981705
Iteration 1945, loss = 1410.30535641
Iteration 1946, loss = 1410.22962003
Iteration 1947, loss = 1410.11490998
Iteration 1948, loss = 1410.06426093
Iteration 1949, loss = 1409.85501266
Iteration 1950, loss = 1409.83706188
Iteration 1951, loss = 1409.91444134
Iteration 1952, loss = 1409.95609236
Iteration 1953, loss = 1410.14262502
Iteration 1954, loss = 1409.84076200
Iteration 1955, loss = 1410.22606190
Iteration 1956, loss = 1409.78900038
Iteration 1957, loss = 1410.06196968
Iteration 1958, loss = 1409.48369109
Iteration 1959, loss = 1409.43663858
Iteration 1960, loss = 1409.38751353
Iteration 1961, loss = 1409.45381489
Iteration 1962, loss = 1409.27704325
Iteration 1963, loss = 1409.36491272
Iteration 1964, loss = 1409.21672912
Iteration 1965, loss = 1409.09614707
Iteration 1966, loss = 1409.04568912
Iteration 1967, loss = 1408.94898944
Iteration 1968, loss = 1408.97766482
Iteration 1969, loss = 1408.97963661
Iteration 1970, loss = 1408.93606080
Iteration 1971, loss = 1408.83402476
Iteration 1972, loss = 1408.71502724
Iteration 1973, loss = 1408.76708677
Iteration 1974, loss = 1408.64702793
Iteration 1975, loss = 1408.76936724
Iteration 1976, loss = 1408.66460144
Iteration 1977, loss = 1408.48052984
Iteration 1978, loss = 1408.38747521
Iteration 1979, loss = 1408.33065404
Iteration 1980, loss = 1408.41329666
Iteration 1981, loss = 1408.41744962
Iteration 1982, loss = 1408.16016339
Iteration 1983, loss = 1408.09335040
Iteration 1984, loss = 1408.21085846
Iteration 1985, loss = 1408.02893668
Iteration 1986, loss = 1408.07115736
Iteration 1987, loss = 1408.02183145
Iteration 1988, loss = 1407.86308256
Iteration 1989, loss = 1407.92306025
Iteration 1990, loss = 1407.83942113
Iteration 1991, loss = 1407.68445595
Iteration 1992, loss = 1407.69685531
Iteration 1993, loss = 1407.83247194
Iteration 1994, loss = 1407.57354718
Iteration 1995, loss = 1407.51905014
Iteration 1996, loss = 1407.49032647
Iteration 1997, loss = 1407.48106512
Iteration 1998, loss = 1407.24893274
Iteration 1999, loss = 1407.23028002
Iteration 2000, loss = 1407.28502983
Iteration 2001, loss = 1407.28937344
Iteration 2002, loss = 1407.18369785
Iteration 2003, loss = 1407.23440911
Iteration 2004, loss = 1407.07650505
Iteration 2005, loss = 1406.93787266
Iteration 2006, loss = 1407.00613301
Iteration 2007, loss = 1406.91259041
Iteration 2008, loss = 1406.77747873
Iteration 2009, loss = 1406.72615705
Iteration 2010, loss = 1406.72652898
Iteration 2011, loss = 1406.79886952
Iteration 2012, loss = 1406.67156645
Iteration 2013, loss = 1406.55479232
Iteration 2014, loss = 1406.59689862
Iteration 2015, loss = 1406.66874447
Iteration 2016, loss = 1406.41717871
Iteration 2017, loss = 1406.30247836
Iteration 2018, loss = 1406.91225162
Iteration 2019, loss = 1406.29877000
Iteration 2020, loss = 1406.32311077
Iteration 2021, loss = 1406.18479832
Iteration 2022, loss = 1406.00915742
Iteration 2023, loss = 1406.09094584
Iteration 2024, loss = 1406.97945537
Iteration 2025, loss = 1406.19481005
Iteration 2026, loss = 1405.86441044
Iteration 2027, loss = 1405.76758581
Iteration 2028, loss = 1405.67372771
Iteration 2029, loss = 1405.77133939
Iteration 2030, loss = 1405.74534662
Iteration 2031, loss = 1405.63080458
Iteration 2032, loss = 1405.55591615
Iteration 2033, loss = 1405.51671253
Iteration 2034, loss = 1405.36541303
Iteration 2035, loss = 1405.29073249
Iteration 2036, loss = 1405.44734143
Iteration 2037, loss = 1405.44044000
Iteration 2038, loss = 1405.17959186
Iteration 2039, loss = 1405.47204046
Iteration 2040, loss = 1405.12736174
Iteration 2041, loss = 1405.29746207
Iteration 2042, loss = 1405.17173326
Iteration 2043, loss = 1405.06680044
Iteration 2044, loss = 1404.91969281
Iteration 2045, loss = 1404.84784219
Iteration 2046, loss = 1404.73017878
Iteration 2047, loss = 1404.65622514
Iteration 2048, loss = 1404.73009320
Iteration 2049, loss = 1404.60118966
Iteration 2050, loss = 1404.49538605
Iteration 2051, loss = 1404.53259836
Iteration 2052, loss = 1404.52749708
Iteration 2053, loss = 1404.44707457
Iteration 2054, loss = 1404.32673809
Iteration 2055, loss = 1404.27237868
Iteration 2056, loss = 1404.22573476
Iteration 2057, loss = 1404.51911242
Iteration 2058, loss = 1404.18678223
Iteration 2059, loss = 1404.05334248
Iteration 2060, loss = 1404.02604263
Iteration 2061, loss = 1403.92258707
Iteration 2062, loss = 1404.03429966
Iteration 2063, loss = 1404.04438801
Iteration 2064, loss = 1403.76062642
Iteration 2065, loss = 1403.73754427
Iteration 2066, loss = 1403.59977737
Iteration 2067, loss = 1403.82497327
Iteration 2068, loss = 1403.59398856
Iteration 2069, loss = 1403.67846539
Iteration 2070, loss = 1403.50594141
Iteration 2071, loss = 1403.49386937
Iteration 2072, loss = 1403.48460704
Iteration 2073, loss = 1403.29170592
Iteration 2074, loss = 1403.24814835
Iteration 2075, loss = 1403.16932967
Iteration 2076, loss = 1403.41154898
Iteration 2077, loss = 1403.07909055
Iteration 2078, loss = 1403.23832052
Iteration 2079, loss = 1402.96556073
Iteration 2080, loss = 1402.92105539
Iteration 2081, loss = 1403.67409718
Iteration 2082, loss = 1402.78289805
Iteration 2083, loss = 1402.78776172
Iteration 2084, loss = 1402.90259267
Iteration 2085, loss = 1402.74573861
Iteration 2086, loss = 1402.68119280
Iteration 2087, loss = 1402.73603532
Iteration 2088, loss = 1402.48395595
Iteration 2089, loss = 1402.38205765
Iteration 2090, loss = 1402.56694501
Iteration 2091, loss = 1402.40354205
Iteration 2092, loss = 1402.37381750
Iteration 2093, loss = 1402.35430366
Iteration 2094, loss = 1402.40511294
Iteration 2095, loss = 1402.17721672
Iteration 2096, loss = 1402.37052434
Iteration 2097, loss = 1402.06639594
Iteration 2098, loss = 1402.14016929
Iteration 2099, loss = 1402.00707025
Iteration 2100, loss = 1402.24621684
Iteration 2101, loss = 1401.80931895
Iteration 2102, loss = 1401.77137291
Iteration 2103, loss = 1401.71498256
Iteration 2104, loss = 1402.31087857
Iteration 2105, loss = 1401.79755098
Iteration 2106, loss = 1401.75213049
Iteration 2107, loss = 1401.91403480
Iteration 2108, loss = 1401.45268111
Iteration 2109, loss = 1401.53749126
Iteration 2110, loss = 1402.16573692
Iteration 2111, loss = 1401.47558743
Iteration 2112, loss = 1401.38367587
Iteration 2113, loss = 1401.34404411
Iteration 2114, loss = 1401.65739436
Iteration 2115, loss = 1401.12675854
Iteration 2116, loss = 1401.08244027
Iteration 2117, loss = 1401.64207774
Iteration 2118, loss = 1401.30076884
Iteration 2119, loss = 1401.02685607
Iteration 2120, loss = 1400.99916139
Iteration 2121, loss = 1400.85982946
Iteration 2122, loss = 1400.75512036
Iteration 2123, loss = 1400.84571590
Iteration 2124, loss = 1400.72846161
Iteration 2125, loss = 1400.60492688
Iteration 2126, loss = 1400.66582448
Iteration 2127, loss = 1400.50602414
Iteration 2128, loss = 1400.49725837
Iteration 2129, loss = 1400.38868789
Iteration 2130, loss = 1400.42061121
Iteration 2131, loss = 1400.30758147
Iteration 2132, loss = 1400.35222013
Iteration 2133, loss = 1400.31378627
Iteration 2134, loss = 1400.15465444
Iteration 2135, loss = 1400.10217019
Iteration 2136, loss = 1400.08326134
Iteration 2137, loss = 1400.13017370
Iteration 2138, loss = 1399.93667012
Iteration 2139, loss = 1399.89027465
Iteration 2140, loss = 1399.88557005
Iteration 2141, loss = 1400.03184773
Iteration 2142, loss = 1399.73507570
Iteration 2143, loss = 1399.93265752
Iteration 2144, loss = 1399.95993518
Iteration 2145, loss = 1399.79053405
Iteration 2146, loss = 1399.68049754
Iteration 2147, loss = 1400.35999719
Iteration 2148, loss = 1399.48466325
Iteration 2149, loss = 1399.38157411
Iteration 2150, loss = 1399.41505128
Iteration 2151, loss = 1399.79767978
Iteration 2152, loss = 1399.47993793
Iteration 2153, loss = 1399.22066387
Iteration 2154, loss = 1399.19265248
Iteration 2155, loss = 1399.26701497
Iteration 2156, loss = 1399.18124940
Iteration 2157, loss = 1399.53876789
Iteration 2158, loss = 1399.05440080
Iteration 2159, loss = 1398.90664753
Iteration 2160, loss = 1398.99722000
Iteration 2161, loss = 1398.83714877
Iteration 2162, loss = 1398.93974681
Iteration 2163, loss = 1398.79805016
Iteration 2164, loss = 1398.84956836
Iteration 2165, loss = 1398.83772157
Iteration 2166, loss = 1398.81140798
Iteration 2167, loss = 1399.01415862
Iteration 2168, loss = 1398.55112657
Iteration 2169, loss = 1398.45232501
Iteration 2170, loss = 1398.68510654
Iteration 2171, loss = 1398.39022274
Iteration 2172, loss = 1398.38356664
Iteration 2173, loss = 1398.27653852
Iteration 2174, loss = 1398.45134151
Iteration 2175, loss = 1398.16576325
Iteration 2176, loss = 1398.14802212
Iteration 2177, loss = 1398.21001488
Iteration 2178, loss = 1398.16115203
Iteration 2179, loss = 1398.09958645
Iteration 2180, loss = 1398.53426381
Iteration 2181, loss = 1397.89550892
Iteration 2182, loss = 1397.88359554
Iteration 2183, loss = 1397.81118552
Iteration 2184, loss = 1397.74489129
Iteration 2185, loss = 1397.75767109
Iteration 2186, loss = 1397.73878422
Iteration 2187, loss = 1397.76628509
Iteration 2188, loss = 1397.78371658
Iteration 2189, loss = 1397.59008982
Iteration 2190, loss = 1397.55800279
Iteration 2191, loss = 1397.41170535
Iteration 2192, loss = 1397.34819385
Iteration 2193, loss = 1397.50791356
Iteration 2194, loss = 1397.45003134
Iteration 2195, loss = 1397.30767090
Iteration 2196, loss = 1397.26133200
Iteration 2197, loss = 1397.16789902
Iteration 2198, loss = 1397.16984940
Iteration 2199, loss = 1397.32397967
Iteration 2200, loss = 1397.12660362
Iteration 2201, loss = 1396.99376427
Iteration 2202, loss = 1396.94137262
Iteration 2203, loss = 1396.79485640
Iteration 2204, loss = 1397.11111885
Iteration 2205, loss = 1396.86005902
Iteration 2206, loss = 1396.87924702
Iteration 2207, loss = 1397.91247266
Iteration 2208, loss = 1396.79146670
Iteration 2209, loss = 1396.71201531
Iteration 2210, loss = 1396.73035312
Iteration 2211, loss = 1396.59278814
Iteration 2212, loss = 1396.52715620
Iteration 2213, loss = 1397.04597781
Iteration 2214, loss = 1396.44566575
Iteration 2215, loss = 1396.38577878
Iteration 2216, loss = 1396.25223560
Iteration 2217, loss = 1396.15778086
Iteration 2218, loss = 1396.18027618
Iteration 2219, loss = 1396.11961034
Iteration 2220, loss = 1396.11970920
Iteration 2221, loss = 1396.00970992
Iteration 2222, loss = 1396.46340968
Iteration 2223, loss = 1395.96126559
Iteration 2224, loss = 1395.91666314
Iteration 2225, loss = 1396.40201359
Iteration 2226, loss = 1395.72497950
Iteration 2227, loss = 1395.82267329
Iteration 2228, loss = 1395.74977383
Iteration 2229, loss = 1395.93464445
Iteration 2230, loss = 1395.67202619
Iteration 2231, loss = 1395.70370526
Iteration 2232, loss = 1395.85721209
Iteration 2233, loss = 1395.80436660
Iteration 2234, loss = 1395.71015132
Iteration 2235, loss = 1395.46844463
Iteration 2236, loss = 1395.23784184
Iteration 2237, loss = 1395.40457335
Iteration 2238, loss = 1395.18846022
Iteration 2239, loss = 1395.11754317
Iteration 2240, loss = 1395.04223869
Iteration 2241, loss = 1395.00358307
Iteration 2242, loss = 1394.96154031
Iteration 2243, loss = 1394.92046673
Iteration 2244, loss = 1394.90627472
Iteration 2245, loss = 1395.18970742
Iteration 2246, loss = 1394.76510910
Iteration 2247, loss = 1395.04292333
Iteration 2248, loss = 1395.05650350
Iteration 2249, loss = 1394.75922128
Iteration 2250, loss = 1394.63356316
Iteration 2251, loss = 1394.61949169
Iteration 2252, loss = 1394.51742220
Iteration 2253, loss = 1394.51160754
Iteration 2254, loss = 1395.47603324
Iteration 2255, loss = 1394.44591323
Iteration 2256, loss = 1394.43026141
Iteration 2257, loss = 1394.23982131
Iteration 2258, loss = 1394.20632049
Iteration 2259, loss = 1394.14587617
Iteration 2260, loss = 1394.12873371
Iteration 2261, loss = 1394.02286368
Iteration 2262, loss = 1393.95214703
Iteration 2263, loss = 1394.13146135
Iteration 2264, loss = 1393.86706187
Iteration 2265, loss = 1393.95526931
Iteration 2266, loss = 1394.12934636
Iteration 2267, loss = 1393.89256041
Iteration 2268, loss = 1393.88454779
Iteration 2269, loss = 1393.79979983
Iteration 2270, loss = 1393.78859835
Iteration 2271, loss = 1393.61966852
Iteration 2272, loss = 1393.55221585
Iteration 2273, loss = 1393.54621159
Iteration 2274, loss = 1393.37229040
Iteration 2275, loss = 1393.37021662
Iteration 2276, loss = 1393.42354725
Iteration 2277, loss = 1393.30849147
Iteration 2278, loss = 1393.40601402
Iteration 2279, loss = 1393.32417263
Iteration 2280, loss = 1393.13893043
Iteration 2281, loss = 1393.03695051
Iteration 2282, loss = 1394.75805729
Iteration 2283, loss = 1392.96479145
Iteration 2284, loss = 1392.90730379
Iteration 2285, loss = 1393.01712693
Iteration 2286, loss = 1392.81628972
Iteration 2287, loss = 1392.78312190
Iteration 2288, loss = 1392.85750342
Iteration 2289, loss = 1392.67926327
Iteration 2290, loss = 1392.76366171
Iteration 2291, loss = 1392.58497588
Iteration 2292, loss = 1392.60618108
Iteration 2293, loss = 1393.07449160
Iteration 2294, loss = 1392.73316026
Iteration 2295, loss = 1392.89049517
Iteration 2296, loss = 1393.26193140
Iteration 2297, loss = 1392.45619737
Iteration 2298, loss = 1392.78340752
Iteration 2299, loss = 1392.26617345
Iteration 2300, loss = 1392.19359275
Iteration 2301, loss = 1392.81936598
Iteration 2302, loss = 1392.15252826
Iteration 2303, loss = 1392.91067898
Iteration 2304, loss = 1391.99224187
Iteration 2305, loss = 1392.52542517
Iteration 2306, loss = 1391.92723352
Iteration 2307, loss = 1391.83576284
Iteration 2308, loss = 1391.78827807
Iteration 2309, loss = 1391.70076504
Iteration 2310, loss = 1391.67927887
Iteration 2311, loss = 1391.87476815
Iteration 2312, loss = 1391.72039880
Iteration 2313, loss = 1391.95157714
Iteration 2314, loss = 1391.51222030
Iteration 2315, loss = 1391.46471038
Iteration 2316, loss = 1391.61353209
Iteration 2317, loss = 1391.55199509
Iteration 2318, loss = 1391.27840696
Iteration 2319, loss = 1391.49704577
Iteration 2320, loss = 1391.24589402
Iteration 2321, loss = 1391.19470957
Iteration 2322, loss = 1391.09739165
Iteration 2323, loss = 1391.04622521
Iteration 2324, loss = 1391.01594485
Iteration 2325, loss = 1391.12133209
Iteration 2326, loss = 1390.96169847
Iteration 2327, loss = 1391.04435826
Iteration 2328, loss = 1390.86690477
Iteration 2329, loss = 1391.01154828
Iteration 2330, loss = 1390.99872050
Iteration 2331, loss = 1390.78676999
Iteration 2332, loss = 1390.67814947
Iteration 2333, loss = 1390.58390326
Iteration 2334, loss = 1390.55244433
Iteration 2335, loss = 1390.53898110
Iteration 2336, loss = 1390.49027609
Iteration 2337, loss = 1390.50492611
Iteration 2338, loss = 1390.40009423
Iteration 2339, loss = 1390.35916692
Iteration 2340, loss = 1390.33106452
Iteration 2341, loss = 1390.41899275
Iteration 2342, loss = 1390.41029577
Iteration 2343, loss = 1390.17334661
Iteration 2344, loss = 1390.14907648
Iteration 2345, loss = 1390.03958310
Iteration 2346, loss = 1390.00999794
Iteration 2347, loss = 1389.98662948
Iteration 2348, loss = 1390.20864004
Iteration 2349, loss = 1389.84929841
Iteration 2350, loss = 1389.93933233
Iteration 2351, loss = 1389.90009248
Iteration 2352, loss = 1389.83366015
Iteration 2353, loss = 1389.83201309
Iteration 2354, loss = 1389.71231190
Iteration 2355, loss = 1389.63209193
Iteration 2356, loss = 1389.64104868
Iteration 2357, loss = 1390.09268467
Iteration 2358, loss = 1389.43572300
Iteration 2359, loss = 1389.38467624
Iteration 2360, loss = 1389.33625779
Iteration 2361, loss = 1389.28229872
Iteration 2362, loss = 1389.23776833
Iteration 2363, loss = 1389.34606810
Iteration 2364, loss = 1389.16867420
Iteration 2365, loss = 1389.10450030
Iteration 2366, loss = 1389.09495663
Iteration 2367, loss = 1389.25610376
Iteration 2368, loss = 1389.02478961
Iteration 2369, loss = 1388.90995235
Iteration 2370, loss = 1388.84051957
Iteration 2371, loss = 1388.79917831
Iteration 2372, loss = 1388.92100061
Iteration 2373, loss = 1388.74589028
Iteration 2374, loss = 1388.70400385
Iteration 2375, loss = 1388.71268828
Iteration 2376, loss = 1388.76600087
Iteration 2377, loss = 1388.64757127
Iteration 2378, loss = 1388.61954502
Iteration 2379, loss = 1390.08304267
Iteration 2380, loss = 1388.71635985
Iteration 2381, loss = 1388.48675589
Iteration 2382, loss = 1388.69800253
Iteration 2383, loss = 1389.43085654
Iteration 2384, loss = 1388.24206345
Iteration 2385, loss = 1388.08477296
Iteration 2386, loss = 1388.18341194
Iteration 2387, loss = 1388.10898276
Iteration 2388, loss = 1388.14471466
Iteration 2389, loss = 1388.18475053
Iteration 2390, loss = 1388.66817652
Iteration 2391, loss = 1388.33751774
Iteration 2392, loss = 1388.03981954
Iteration 2393, loss = 1387.79451541
Iteration 2394, loss = 1387.86137185
Iteration 2395, loss = 1387.70606638
Iteration 2396, loss = 1387.71566686
Iteration 2397, loss = 1387.79805260
Iteration 2398, loss = 1387.65655059
Iteration 2399, loss = 1387.55466488
Iteration 2400, loss = 1387.81544098
Iteration 2401, loss = 1387.55141976
Iteration 2402, loss = 1387.42936173
Iteration 2403, loss = 1387.33442199
Iteration 2404, loss = 1387.41730080
Iteration 2405, loss = 1387.32694054
Iteration 2406, loss = 1387.22468165
Iteration 2407, loss = 1387.17733680
Iteration 2408, loss = 1387.27527335
Iteration 2409, loss = 1387.12928828
Iteration 2410, loss = 1387.07114940
Iteration 2411, loss = 1387.01936351
Iteration 2412, loss = 1387.18657022
Iteration 2413, loss = 1386.96234815
Iteration 2414, loss = 1386.84770414
Iteration 2415, loss = 1387.02646445
Iteration 2416, loss = 1386.73816890
Iteration 2417, loss = 1386.71168844
Iteration 2418, loss = 1386.87153010
Iteration 2419, loss = 1386.69063684
Iteration 2420, loss = 1386.98387906
Iteration 2421, loss = 1386.57000460
Iteration 2422, loss = 1386.49771662
Iteration 2423, loss = 1386.74823076
Iteration 2424, loss = 1386.46627984
Iteration 2425, loss = 1386.80222634
Iteration 2426, loss = 1386.35494501
Iteration 2427, loss = 1386.67888250
Iteration 2428, loss = 1386.42226937
Iteration 2429, loss = 1386.28819158
Iteration 2430, loss = 1386.27115995
Iteration 2431, loss = 1386.15630605
Iteration 2432, loss = 1386.06022621
Iteration 2433, loss = 1386.62805641
Iteration 2434, loss = 1386.09152764
Iteration 2435, loss = 1385.93924286
Iteration 2436, loss = 1386.09895064
Iteration 2437, loss = 1385.86810191
Iteration 2438, loss = 1385.80535633
Iteration 2439, loss = 1385.88735650
Iteration 2440, loss = 1385.91990771
Iteration 2441, loss = 1385.74317284
Iteration 2442, loss = 1385.70292020
Iteration 2443, loss = 1386.48243363
Iteration 2444, loss = 1385.75550448
Iteration 2445, loss = 1385.55834432
Iteration 2446, loss = 1385.44352684
Iteration 2447, loss = 1385.67305908
Iteration 2448, loss = 1385.49373664
Iteration 2449, loss = 1385.77634670
Iteration 2450, loss = 1385.39269535
Iteration 2451, loss = 1385.33558491
Iteration 2452, loss = 1385.19213025
Iteration 2453, loss = 1385.20154259
Iteration 2454, loss = 1385.45476585
Iteration 2455, loss = 1385.47796087
Iteration 2456, loss = 1385.11161638
Iteration 2457, loss = 1384.96538962
Iteration 2458, loss = 1384.94204518
Iteration 2459, loss = 1384.85020331
Iteration 2460, loss = 1384.91135567
Iteration 2461, loss = 1384.99647639
Iteration 2462, loss = 1384.82298892
Iteration 2463, loss = 1385.01095705
Iteration 2464, loss = 1384.84698270
Iteration 2465, loss = 1384.78979026
Iteration 2466, loss = 1384.66194828
Iteration 2467, loss = 1384.54629694
Iteration 2468, loss = 1384.61278182
Iteration 2469, loss = 1384.51137690
Iteration 2470, loss = 1384.67619871
Iteration 2471, loss = 1384.36315943
Iteration 2472, loss = 1384.46439830
Iteration 2473, loss = 1384.64373797
Iteration 2474, loss = 1384.29853592
Iteration 2475, loss = 1384.32431131
Iteration 2476, loss = 1384.21677074
Iteration 2477, loss = 1384.25473307
Iteration 2478, loss = 1384.18566701
Iteration 2479, loss = 1384.03725926
Iteration 2480, loss = 1384.09635307
Iteration 2481, loss = 1383.97619290
Iteration 2482, loss = 1383.99948552
Iteration 2483, loss = 1383.86273975
Iteration 2484, loss = 1383.80807000
Iteration 2485, loss = 1383.92905381
Iteration 2486, loss = 1383.76883200
Iteration 2487, loss = 1383.71735497
Iteration 2488, loss = 1383.94969308
Iteration 2489, loss = 1383.69256821
Iteration 2490, loss = 1383.67816975
Iteration 2491, loss = 1384.17398687
Iteration 2492, loss = 1383.47177435
Iteration 2493, loss = 1383.58678155
Iteration 2494, loss = 1383.43374420
Iteration 2495, loss = 1383.61542015
Iteration 2496, loss = 1383.74464980
Iteration 2497, loss = 1383.47357289
Iteration 2498, loss = 1383.38900084
Iteration 2499, loss = 1383.27852542
Iteration 2500, loss = 1383.54420100
Iteration 2501, loss = 1383.22050282
Iteration 2502, loss = 1383.40986289
Iteration 2503, loss = 1383.08912136
Iteration 2504, loss = 1383.03127791
Iteration 2505, loss = 1383.06120123
Iteration 2506, loss = 1383.06697984
Iteration 2507, loss = 1383.03184378
Iteration 2508, loss = 1383.13873100
Iteration 2509, loss = 1382.98034910
Iteration 2510, loss = 1382.91053086
Iteration 2511, loss = 1382.88694931
Iteration 2512, loss = 1383.00084291
Iteration 2513, loss = 1382.73970784
Iteration 2514, loss = 1382.60157123
Iteration 2515, loss = 1382.59434868
Iteration 2516, loss = 1382.59031254
Iteration 2517, loss = 1382.48590552
Iteration 2518, loss = 1382.57484224
Iteration 2519, loss = 1382.70915821
Iteration 2520, loss = 1382.59799081
Iteration 2521, loss = 1382.55683366
Iteration 2522, loss = 1382.48483452
Iteration 2523, loss = 1382.21624266
Iteration 2524, loss = 1382.17308409
Iteration 2525, loss = 1382.48637423
Iteration 2526, loss = 1382.27108852
Iteration 2527, loss = 1382.42136831
Iteration 2528, loss = 1382.19184566
Iteration 2529, loss = 1382.04900796
Iteration 2530, loss = 1381.94774173
Iteration 2531, loss = 1381.88015096
Iteration 2532, loss = 1381.92128652
Iteration 2533, loss = 1381.96892318
Iteration 2534, loss = 1381.95735633
Iteration 2535, loss = 1381.89818952
Iteration 2536, loss = 1382.19944844
Iteration 2537, loss = 1382.23239576
Iteration 2538, loss = 1381.69496932
Iteration 2539, loss = 1381.69446899
Iteration 2540, loss = 1381.63612926
Iteration 2541, loss = 1381.76463258
Iteration 2542, loss = 1381.69412036
Iteration 2543, loss = 1381.46359858
Iteration 2544, loss = 1381.68828361
Iteration 2545, loss = 1381.41123549
Iteration 2546, loss = 1381.97755299
Iteration 2547, loss = 1381.33410163
Iteration 2548, loss = 1381.32093372
Iteration 2549, loss = 1381.23723255
Iteration 2550, loss = 1381.23068379
Iteration 2551, loss = 1381.16066604
Iteration 2552, loss = 1381.77874109
Iteration 2553, loss = 1381.53823685
Iteration 2554, loss = 1381.31328666
Iteration 2555, loss = 1381.00073595
Iteration 2556, loss = 1380.94703047
Iteration 2557, loss = 1380.96601819
Iteration 2558, loss = 1380.87372494
Iteration 2559, loss = 1380.83505456
Iteration 2560, loss = 1380.82150724
Iteration 2561, loss = 1381.01626222
Iteration 2562, loss = 1380.88267224
Iteration 2563, loss = 1380.84948735
Iteration 2564, loss = 1380.66813057
Iteration 2565, loss = 1380.63883498
Iteration 2566, loss = 1380.60630448
Iteration 2567, loss = 1380.52337330
Iteration 2568, loss = 1380.63833022
Iteration 2569, loss = 1380.65146284
Iteration 2570, loss = 1380.44172847
Iteration 2571, loss = 1380.66063510
Iteration 2572, loss = 1380.41616990
Iteration 2573, loss = 1380.79540062
Iteration 2574, loss = 1380.32834145
Iteration 2575, loss = 1380.26046034
Iteration 2576, loss = 1380.24212703
Iteration 2577, loss = 1380.37111938
Iteration 2578, loss = 1380.15790910
Iteration 2579, loss = 1380.02694575
Iteration 2580, loss = 1379.99602927
Iteration 2581, loss = 1380.11900995
Iteration 2582, loss = 1380.02305729
Iteration 2583, loss = 1380.00126402
Iteration 2584, loss = 1379.99604972
Iteration 2585, loss = 1380.02411530
Iteration 2586, loss = 1379.79924573
Iteration 2587, loss = 1380.35598295
Iteration 2588, loss = 1380.87005225
Iteration 2589, loss = 1379.73552040
Iteration 2590, loss = 1379.70444612
Iteration 2591, loss = 1379.71957326
Iteration 2592, loss = 1379.74194302
Iteration 2593, loss = 1379.75111420
Iteration 2594, loss = 1380.12249892
Iteration 2595, loss = 1379.75177997
Iteration 2596, loss = 1379.89504324
Iteration 2597, loss = 1380.10801309
Iteration 2598, loss = 1379.82255485
Iteration 2599, loss = 1379.47329964
Iteration 2600, loss = 1379.33586854
Iteration 2601, loss = 1379.27625335
Iteration 2602, loss = 1379.20821150
Iteration 2603, loss = 1379.16091845
Iteration 2604, loss = 1379.27340138
Iteration 2605, loss = 1379.13431830
Iteration 2606, loss = 1379.27107403
Iteration 2607, loss = 1379.18569719
Iteration 2608, loss = 1379.09993888
Iteration 2609, loss = 1379.02618521
Iteration 2610, loss = 1378.96248789
Iteration 2611, loss = 1379.00811664
Iteration 2612, loss = 1378.98775546
Iteration 2613, loss = 1378.79140404
Iteration 2614, loss = 1379.65984391
Iteration 2615, loss = 1378.84965658
Iteration 2616, loss = 1378.77113848
Iteration 2617, loss = 1378.69266986
Iteration 2618, loss = 1378.69156709
Iteration 2619, loss = 1378.68347343
Iteration 2620, loss = 1378.56276032
Iteration 2621, loss = 1378.50404011
Iteration 2622, loss = 1378.60916696
Iteration 2623, loss = 1378.53190948
Iteration 2624, loss = 1378.63482451
Iteration 2625, loss = 1378.89854150
Iteration 2626, loss = 1378.35032254
Iteration 2627, loss = 1378.23268376
Iteration 2628, loss = 1378.23485931
Iteration 2629, loss = 1378.77857642
Iteration 2630, loss = 1378.67692334
Iteration 2631, loss = 1378.85199534
Iteration 2632, loss = 1378.31040190
Iteration 2633, loss = 1378.47182844
Iteration 2634, loss = 1378.20776096
Iteration 2635, loss = 1378.17068552
Iteration 2636, loss = 1378.19635736
Iteration 2637, loss = 1378.02653893
Iteration 2638, loss = 1377.95654838
Iteration 2639, loss = 1377.86778082
Iteration 2640, loss = 1377.83332839
Iteration 2641, loss = 1377.84656905
Iteration 2642, loss = 1377.83551924
Iteration 2643, loss = 1377.82644616
Iteration 2644, loss = 1377.76302739
Iteration 2645, loss = 1377.75323389
Iteration 2646, loss = 1377.64578646
Iteration 2647, loss = 1377.65597783
Iteration 2648, loss = 1377.63019719
Iteration 2649, loss = 1377.59388106
Iteration 2650, loss = 1377.77669498
Iteration 2651, loss = 1377.63337340
Iteration 2652, loss = 1377.43008190
Iteration 2653, loss = 1377.41373777
Iteration 2654, loss = 1377.86912797
Iteration 2655, loss = 1377.43924964
Iteration 2656, loss = 1377.91045789
Iteration 2657, loss = 1377.50619034
Iteration 2658, loss = 1377.26956893
Iteration 2659, loss = 1377.34042091
Iteration 2660, loss = 1377.20101460
Iteration 2661, loss = 1377.29179752
Iteration 2662, loss = 1377.64373758
Iteration 2663, loss = 1377.06673126
Iteration 2664, loss = 1377.07870296
Iteration 2665, loss = 1377.01167888
Iteration 2666, loss = 1377.00960180
Iteration 2667, loss = 1376.96043876
Iteration 2668, loss = 1377.35004892
Iteration 2669, loss = 1376.92702889
Iteration 2670, loss = 1376.92187507
Iteration 2671, loss = 1376.78915416
Iteration 2672, loss = 1376.92226027
Iteration 2673, loss = 1376.69394180
Iteration 2674, loss = 1376.80928468
Iteration 2675, loss = 1376.78464793
Iteration 2676, loss = 1376.64921404
Iteration 2677, loss = 1376.60141833
Iteration 2678, loss = 1376.56933085
Iteration 2679, loss = 1376.53944904
Iteration 2680, loss = 1376.61073985
Iteration 2681, loss = 1376.50224450
Iteration 2682, loss = 1376.50733027
Iteration 2683, loss = 1376.37282700
Iteration 2684, loss = 1376.71760923
Iteration 2685, loss = 1376.42341642
Iteration 2686, loss = 1376.31779055
Iteration 2687, loss = 1376.24036901
Iteration 2688, loss = 1376.58586043
Iteration 2689, loss = 1376.69805267
Iteration 2690, loss = 1376.39019517
Iteration 2691, loss = 1376.40851133
Iteration 2692, loss = 1376.83864557
Iteration 2693, loss = 1376.09033073
Iteration 2694, loss = 1376.07271174
Iteration 2695, loss = 1376.04521686
Iteration 2696, loss = 1376.03627583
Iteration 2697, loss = 1375.93038700
Iteration 2698, loss = 1375.87546187
Iteration 2699, loss = 1375.82051568
Iteration 2700, loss = 1375.78861615
Iteration 2701, loss = 1375.75739555
Iteration 2702, loss = 1375.87109288
Iteration 2703, loss = 1375.96608744
Iteration 2704, loss = 1375.68484309
Iteration 2705, loss = 1375.70828661
Iteration 2706, loss = 1375.61038764
Iteration 2707, loss = 1376.02573581
Iteration 2708, loss = 1375.59165601
Iteration 2709, loss = 1375.79367503
Iteration 2710, loss = 1375.43204592
Iteration 2711, loss = 1375.56204040
Iteration 2712, loss = 1375.52367140
Iteration 2713, loss = 1376.12586222
Iteration 2714, loss = 1375.37353244
Iteration 2715, loss = 1375.38427300
Iteration 2716, loss = 1375.41947565
Iteration 2717, loss = 1375.45776975
Iteration 2718, loss = 1375.38145495
Iteration 2719, loss = 1375.34526576
Iteration 2720, loss = 1375.34099038
Iteration 2721, loss = 1375.66800098
Iteration 2722, loss = 1375.42833845
Iteration 2723, loss = 1375.85462780
Iteration 2724, loss = 1375.08651370
Iteration 2725, loss = 1375.22877655
Iteration 2726, loss = 1375.02342799
Iteration 2727, loss = 1374.90120480
Iteration 2728, loss = 1375.00564199
Iteration 2729, loss = 1374.83898879
Iteration 2730, loss = 1374.83740240
Iteration 2731, loss = 1375.02239946
Iteration 2732, loss = 1374.86379836
Iteration 2733, loss = 1375.49285839
Iteration 2734, loss = 1374.79099862
Iteration 2735, loss = 1374.91373478
Iteration 2736, loss = 1374.67086880
Iteration 2737, loss = 1374.65775021
Iteration 2738, loss = 1374.69601775
Iteration 2739, loss = 1374.54364833
Iteration 2740, loss = 1374.71049582
Iteration 2741, loss = 1374.71274996
Iteration 2742, loss = 1374.43330600
Iteration 2743, loss = 1374.55941992
Iteration 2744, loss = 1374.41122948
Iteration 2745, loss = 1374.36239090
Iteration 2746, loss = 1374.57908429
Iteration 2747, loss = 1374.28522619
Iteration 2748, loss = 1374.40350007
Iteration 2749, loss = 1374.32914865
Iteration 2750, loss = 1374.26720851
Iteration 2751, loss = 1374.22908043
Iteration 2752, loss = 1374.15042772
Iteration 2753, loss = 1374.14133246
Iteration 2754, loss = 1374.08993751
Iteration 2755, loss = 1374.17464561
Iteration 2756, loss = 1374.00414869
Iteration 2757, loss = 1373.98180054
Iteration 2758, loss = 1373.95901413
Iteration 2759, loss = 1373.89643628
Iteration 2760, loss = 1374.00261697
Iteration 2761, loss = 1373.83984890
Iteration 2762, loss = 1373.92943245
Iteration 2763, loss = 1373.85296745
Iteration 2764, loss = 1374.15539625
Iteration 2765, loss = 1373.69341641
Iteration 2766, loss = 1373.78507498
Iteration 2767, loss = 1373.66486909
Iteration 2768, loss = 1373.63398055
Iteration 2769, loss = 1374.27035834
Iteration 2770, loss = 1373.64991762
Iteration 2771, loss = 1373.62436225
Iteration 2772, loss = 1373.47600387
Iteration 2773, loss = 1373.59974198
Iteration 2774, loss = 1373.78308173
Iteration 2775, loss = 1373.57277517
Iteration 2776, loss = 1373.36293874
Iteration 2777, loss = 1373.40367165
Iteration 2778, loss = 1373.25890868
Iteration 2779, loss = 1373.20592376
Iteration 2780, loss = 1373.27353417
Iteration 2781, loss = 1373.90109562
Iteration 2782, loss = 1373.29307019
Iteration 2783, loss = 1373.16726662
Iteration 2784, loss = 1373.13358892
Iteration 2785, loss = 1373.01366468
Iteration 2786, loss = 1373.03982544
Iteration 2787, loss = 1372.99342785
Iteration 2788, loss = 1373.09875776
Iteration 2789, loss = 1373.05412720
Iteration 2790, loss = 1373.08088511
Iteration 2791, loss = 1373.07854416
Iteration 2792, loss = 1373.08856585
Iteration 2793, loss = 1372.85711496
Iteration 2794, loss = 1373.10386974
Iteration 2795, loss = 1372.77422415
Iteration 2796, loss = 1372.74509580
Iteration 2797, loss = 1372.74785743
Iteration 2798, loss = 1372.79589913
Iteration 2799, loss = 1373.43082602
Iteration 2800, loss = 1372.60384956
Iteration 2801, loss = 1372.53183667
Iteration 2802, loss = 1372.65520051
Iteration 2803, loss = 1372.68594346
Iteration 2804, loss = 1373.61232147
Iteration 2805, loss = 1372.70104003
Iteration 2806, loss = 1372.63181647
Iteration 2807, loss = 1372.87177186
Iteration 2808, loss = 1373.18790047
Iteration 2809, loss = 1372.60201806
Iteration 2810, loss = 1372.76923767
Iteration 2811, loss = 1372.42038799
Iteration 2812, loss = 1372.22363548
Iteration 2813, loss = 1372.15379604
Iteration 2814, loss = 1372.22240136
Iteration 2815, loss = 1372.25061178
Iteration 2816, loss = 1372.81027243
Iteration 2817, loss = 1372.23379604
Iteration 2818, loss = 1372.28686850
Iteration 2819, loss = 1372.25845940
Iteration 2820, loss = 1372.30528485
Iteration 2821, loss = 1372.06427980
Iteration 2822, loss = 1371.90512368
Iteration 2823, loss = 1371.91063511
Iteration 2824, loss = 1371.84160888
Iteration 2825, loss = 1371.79452424
Iteration 2826, loss = 1372.10811927
Iteration 2827, loss = 1371.86384656
Iteration 2828, loss = 1371.78697482
Iteration 2829, loss = 1371.81309975
Iteration 2830, loss = 1371.79368863
Iteration 2831, loss = 1371.94989273
Iteration 2832, loss = 1371.79029168
Iteration 2833, loss = 1372.08809520
Iteration 2834, loss = 1371.58171238
Iteration 2835, loss = 1371.52087987
Iteration 2836, loss = 1371.67935818
Iteration 2837, loss = 1371.52155862
Iteration 2838, loss = 1371.57322903
Iteration 2839, loss = 1371.52441198
Iteration 2840, loss = 1371.36867087
Iteration 2841, loss = 1371.32505688
Iteration 2842, loss = 1371.31245028
Iteration 2843, loss = 1371.42857383
Iteration 2844, loss = 1371.38529074
Iteration 2845, loss = 1371.55135342
Iteration 2846, loss = 1371.25007747
Iteration 2847, loss = 1371.18664284
Iteration 2848, loss = 1371.31174750
Iteration 2849, loss = 1371.16408047
Iteration 2850, loss = 1371.27420313
Iteration 2851, loss = 1371.13948008
Iteration 2852, loss = 1371.03285917
Iteration 2853, loss = 1371.11433434
Iteration 2854, loss = 1370.96379034
Iteration 2855, loss = 1370.95117852
Iteration 2856, loss = 1370.98432090
Iteration 2857, loss = 1370.98123423
Iteration 2858, loss = 1371.19751975
Iteration 2859, loss = 1370.84894259
Iteration 2860, loss = 1370.78888359
Iteration 2861, loss = 1370.85994773
Iteration 2862, loss = 1370.76379085
Iteration 2863, loss = 1370.73248962
Iteration 2864, loss = 1370.66790490
Iteration 2865, loss = 1370.65698765
Iteration 2866, loss = 1370.62490978
Iteration 2867, loss = 1370.59384835
Iteration 2868, loss = 1370.60925009
Iteration 2869, loss = 1370.54632363
Iteration 2870, loss = 1370.54825919
Iteration 2871, loss = 1370.47032425
Iteration 2872, loss = 1370.64861509
Iteration 2873, loss = 1371.00432959
Iteration 2874, loss = 1370.55377391
Iteration 2875, loss = 1370.56777679
Iteration 2876, loss = 1370.39649505
Iteration 2877, loss = 1370.54798081
Iteration 2878, loss = 1370.35187363
Iteration 2879, loss = 1370.41330718
Iteration 2880, loss = 1370.55949975
Iteration 2881, loss = 1370.43838209
Iteration 2882, loss = 1370.32449683
Iteration 2883, loss = 1370.26882371
Iteration 2884, loss = 1370.10499216
Iteration 2885, loss = 1370.10293249
Iteration 2886, loss = 1370.08975350
Iteration 2887, loss = 1370.38733030
Iteration 2888, loss = 1370.35572232
Iteration 2889, loss = 1370.11701690
Iteration 2890, loss = 1370.17156547
Iteration 2891, loss = 1370.67868192
Iteration 2892, loss = 1369.84835247
Iteration 2893, loss = 1369.81887079
Iteration 2894, loss = 1370.19400830
Iteration 2895, loss = 1370.37814895
Iteration 2896, loss = 1369.96666375
Iteration 2897, loss = 1370.74398754
Iteration 2898, loss = 1369.77146773
Iteration 2899, loss = 1369.67778849
Iteration 2900, loss = 1369.61107907
Iteration 2901, loss = 1369.77946434
Iteration 2902, loss = 1369.71034403
Iteration 2903, loss = 1369.69069387
Iteration 2904, loss = 1369.61311382
Iteration 2905, loss = 1369.88908742
Iteration 2906, loss = 1369.60485940
Iteration 2907, loss = 1369.89248406
Iteration 2908, loss = 1369.49207989
Iteration 2909, loss = 1369.50733823
Iteration 2910, loss = 1369.63308573
Iteration 2911, loss = 1369.38929114
Iteration 2912, loss = 1369.45473388
Iteration 2913, loss = 1369.36865285
Iteration 2914, loss = 1369.36511450
Iteration 2915, loss = 1370.88706320
Iteration 2916, loss = 1369.34508460
Iteration 2917, loss = 1369.45984787
Iteration 2918, loss = 1369.63844930
Iteration 2919, loss = 1369.15146173
Iteration 2920, loss = 1369.11246973
Iteration 2921, loss = 1369.06524828
Iteration 2922, loss = 1369.56402438
Iteration 2923, loss = 1369.26154830
Iteration 2924, loss = 1369.66184249
Iteration 2925, loss = 1369.00662499
Iteration 2926, loss = 1368.98775535
Iteration 2927, loss = 1369.00035285
Iteration 2928, loss = 1369.03475998
Iteration 2929, loss = 1369.00316647
Iteration 2930, loss = 1369.05010437
Iteration 2931, loss = 1369.11791678
Iteration 2932, loss = 1369.17324314
Iteration 2933, loss = 1368.77719250
Iteration 2934, loss = 1368.81915157
Iteration 2935, loss = 1368.96776110
Iteration 2936, loss = 1368.71173668
Iteration 2937, loss = 1368.76056974
Iteration 2938, loss = 1368.80347594
Iteration 2939, loss = 1368.98781728
Iteration 2940, loss = 1369.24256547
Iteration 2941, loss = 1368.57484549
Iteration 2942, loss = 1368.57748998
Iteration 2943, loss = 1368.53919700
Iteration 2944, loss = 1369.08874593
Iteration 2945, loss = 1368.60993448
Iteration 2946, loss = 1368.44694433
Iteration 2947, loss = 1368.40501378
Iteration 2948, loss = 1368.41831289
Iteration 2949, loss = 1368.64023257
Iteration 2950, loss = 1368.40055920
Iteration 2951, loss = 1368.33589658
Iteration 2952, loss = 1368.38052586
Iteration 2953, loss = 1368.61964762
Iteration 2954, loss = 1368.39827441
Iteration 2955, loss = 1368.23279638
Iteration 2956, loss = 1368.63728537
Iteration 2957, loss = 1368.39180214
Iteration 2958, loss = 1368.12483289
Iteration 2959, loss = 1368.11877648
Iteration 2960, loss = 1368.25851812
Iteration 2961, loss = 1369.10906484
Iteration 2962, loss = 1368.05293794
Iteration 2963, loss = 1368.07445178
Iteration 2964, loss = 1368.04149926
Iteration 2965, loss = 1368.13709700
Iteration 2966, loss = 1368.40505200
Iteration 2967, loss = 1367.97195861
Iteration 2968, loss = 1367.91154934
Iteration 2969, loss = 1367.95041800
Iteration 2970, loss = 1367.95012089
Iteration 2971, loss = 1367.82127703
Iteration 2972, loss = 1368.01609491
Iteration 2973, loss = 1367.74519946
Iteration 2974, loss = 1368.79015600
Iteration 2975, loss = 1367.86469187
Iteration 2976, loss = 1367.75534985
Iteration 2977, loss = 1368.13933185
Iteration 2978, loss = 1367.89572698
Iteration 2979, loss = 1367.72358581
Iteration 2980, loss = 1368.03415709
Iteration 2981, loss = 1367.70797246
Iteration 2982, loss = 1367.52770595
Iteration 2983, loss = 1368.36083447
Iteration 2984, loss = 1367.56143818
Iteration 2985, loss = 1367.46162208
Iteration 2986, loss = 1368.80592688
Iteration 2987, loss = 1367.56361254
Iteration 2988, loss = 1367.44412196
Iteration 2989, loss = 1367.36206266
Iteration 2990, loss = 1367.42159610
Iteration 2991, loss = 1367.27521822
Iteration 2992, loss = 1367.62369028
Iteration 2993, loss = 1367.20838484
Iteration 2994, loss = 1367.28885754
Iteration 2995, loss = 1367.45294137
Iteration 2996, loss = 1367.28246016
Iteration 2997, loss = 1367.59078308
Iteration 2998, loss = 1367.48780788
Iteration 2999, loss = 1367.29066864
Iteration 3000, loss = 1367.10893905
Iteration 3001, loss = 1367.04319805
Iteration 3002, loss = 1367.13802814
Iteration 3003, loss = 1367.16296594
Iteration 3004, loss = 1367.11860578
Iteration 3005, loss = 1367.21881123
Iteration 3006, loss = 1367.33115277
Iteration 3007, loss = 1367.25369533
Iteration 3008, loss = 1367.08053764
Iteration 3009, loss = 1367.03588187
Iteration 3010, loss = 1366.85763851
Iteration 3011, loss = 1366.89687833
Iteration 3012, loss = 1367.62179671
Iteration 3013, loss = 1366.85560156
Iteration 3014, loss = 1367.07138989
Iteration 3015, loss = 1366.81391029
Iteration 3016, loss = 1366.96809696
Iteration 3017, loss = 1366.78283283
Iteration 3018, loss = 1366.90685980
Iteration 3019, loss = 1366.63333152
Iteration 3020, loss = 1366.65984440
Iteration 3021, loss = 1366.59917053
Iteration 3022, loss = 1366.56021223
Iteration 3023, loss = 1366.64637596
Iteration 3024, loss = 1366.56033344
Iteration 3025, loss = 1366.44825488
Iteration 3026, loss = 1366.54558356
Iteration 3027, loss = 1367.08116004
Iteration 3028, loss = 1366.51262640
Iteration 3029, loss = 1366.41997125
Iteration 3030, loss = 1366.37959698
Iteration 3031, loss = 1366.30408320
Iteration 3032, loss = 1366.28908228
Iteration 3033, loss = 1366.28706334
Iteration 3034, loss = 1366.19398399
Iteration 3035, loss = 1366.38244861
Iteration 3036, loss = 1366.20930045
Iteration 3037, loss = 1366.38281501
Iteration 3038, loss = 1366.12016952
Iteration 3039, loss = 1366.19845446
Iteration 3040, loss = 1366.20706094
Iteration 3041, loss = 1366.20275160
Iteration 3042, loss = 1366.11176708
Iteration 3043, loss = 1366.10045611
Iteration 3044, loss = 1366.02318701
Iteration 3045, loss = 1366.01855507
Iteration 3046, loss = 1365.90486583
Iteration 3047, loss = 1365.91061836
Iteration 3048, loss = 1365.83642858
Iteration 3049, loss = 1365.89223497
Iteration 3050, loss = 1365.82561877
Iteration 3051, loss = 1365.86350161
Iteration 3052, loss = 1366.06665501
Iteration 3053, loss = 1365.80316042
Iteration 3054, loss = 1366.06665686
Iteration 3055, loss = 1365.66192492
Iteration 3056, loss = 1365.59177378
Iteration 3057, loss = 1365.61492934
Iteration 3058, loss = 1365.83236518
Iteration 3059, loss = 1365.61399892
Iteration 3060, loss = 1365.67044194
Iteration 3061, loss = 1365.81771240
Iteration 3062, loss = 1366.02688964
Iteration 3063, loss = 1365.55036976
Iteration 3064, loss = 1365.92031400
Iteration 3065, loss = 1365.47575066
Iteration 3066, loss = 1365.43794695
Iteration 3067, loss = 1365.53750585
Iteration 3068, loss = 1365.33067728
Iteration 3069, loss = 1366.57857275
Iteration 3070, loss = 1365.32936270
Iteration 3071, loss = 1365.23729821
Iteration 3072, loss = 1365.86568063
Iteration 3073, loss = 1365.26868135
Iteration 3074, loss = 1365.49655629
Iteration 3075, loss = 1365.18873622
Iteration 3076, loss = 1365.18002677
Iteration 3077, loss = 1365.51424242
Iteration 3078, loss = 1365.10241940
Iteration 3079, loss = 1365.08637193
Iteration 3080, loss = 1365.13463984
Iteration 3081, loss = 1365.29540700
Iteration 3082, loss = 1364.98294431
Iteration 3083, loss = 1365.17911008
Iteration 3084, loss = 1365.22703945
Iteration 3085, loss = 1365.04369158
Iteration 3086, loss = 1364.91592311
Iteration 3087, loss = 1364.85738256
Iteration 3088, loss = 1364.80718969
Iteration 3089, loss = 1365.13146859
Iteration 3090, loss = 1364.74465547
Iteration 3091, loss = 1365.52620675
Iteration 3092, loss = 1365.09557126
Iteration 3093, loss = 1364.78824416
Iteration 3094, loss = 1364.74072837
Iteration 3095, loss = 1364.73282604
Iteration 3096, loss = 1364.64744891
Iteration 3097, loss = 1364.94180023
Iteration 3098, loss = 1364.70367202
Iteration 3099, loss = 1364.78464697
Iteration 3100, loss = 1364.84840523
Iteration 3101, loss = 1364.63647481
Iteration 3102, loss = 1364.79311225
Iteration 3103, loss = 1364.51074921
Iteration 3104, loss = 1364.62450158
Iteration 3105, loss = 1364.33002346
Iteration 3106, loss = 1364.51928960
Iteration 3107, loss = 1364.65920283
Iteration 3108, loss = 1364.62109859
Iteration 3109, loss = 1364.49909162
Iteration 3110, loss = 1364.37131554
Iteration 3111, loss = 1364.41878689
Iteration 3112, loss = 1365.25426209
Iteration 3113, loss = 1364.23873293
Iteration 3114, loss = 1364.22905354
Iteration 3115, loss = 1364.19422428
Iteration 3116, loss = 1364.30126755
Iteration 3117, loss = 1364.13521363
Iteration 3118, loss = 1364.13500041
Iteration 3119, loss = 1364.13389740
Iteration 3120, loss = 1364.13429688
Iteration 3121, loss = 1364.28997014
Iteration 3122, loss = 1363.99850610
Iteration 3123, loss = 1364.30596939
Iteration 3124, loss = 1364.11838189
Iteration 3125, loss = 1364.36151880
Iteration 3126, loss = 1364.04414864
Iteration 3127, loss = 1364.10217681
Iteration 3128, loss = 1364.00376195
Iteration 3129, loss = 1363.91259764
Iteration 3130, loss = 1363.94593989
Iteration 3131, loss = 1363.94137023
Iteration 3132, loss = 1363.78698413
Iteration 3133, loss = 1363.84231283
Iteration 3134, loss = 1363.87425669
Iteration 3135, loss = 1363.80886807
Iteration 3136, loss = 1363.81546416
Iteration 3137, loss = 1363.82307601
Iteration 3138, loss = 1363.74229454
Iteration 3139, loss = 1363.78824985
Iteration 3140, loss = 1363.72183300
Iteration 3141, loss = 1363.72962857
Iteration 3142, loss = 1363.60565749
Iteration 3143, loss = 1363.61274605
Iteration 3144, loss = 1363.58740273
Iteration 3145, loss = 1363.51220340
Iteration 3146, loss = 1363.99738170
Iteration 3147, loss = 1363.91612316
Iteration 3148, loss = 1363.63167380
Iteration 3149, loss = 1363.39141425
Iteration 3150, loss = 1363.39363137
Iteration 3151, loss = 1363.41292563
Iteration 3152, loss = 1363.39205283
Iteration 3153, loss = 1363.51800212
Iteration 3154, loss = 1363.63254870
Iteration 3155, loss = 1363.30432141
Iteration 3156, loss = 1363.19853082
Iteration 3157, loss = 1363.29628765
Iteration 3158, loss = 1363.35524446
Iteration 3159, loss = 1363.21480882
Iteration 3160, loss = 1363.16583803
Iteration 3161, loss = 1363.15680390
Iteration 3162, loss = 1363.06655334
Iteration 3163, loss = 1363.53284362
Iteration 3164, loss = 1363.28135716
Iteration 3165, loss = 1363.29731157
Iteration 3166, loss = 1363.15052587
Iteration 3167, loss = 1363.59201921
Iteration 3168, loss = 1363.31560393
Iteration 3169, loss = 1363.02256503
Iteration 3170, loss = 1362.93197512
Iteration 3171, loss = 1362.97570723
Iteration 3172, loss = 1363.24083690
Iteration 3173, loss = 1362.88438141
Iteration 3174, loss = 1363.30930643
Iteration 3175, loss = 1362.86612423
Iteration 3176, loss = 1363.33905326
Iteration 3177, loss = 1362.71430728
Iteration 3178, loss = 1363.21675152
Iteration 3179, loss = 1362.71296508
Iteration 3180, loss = 1362.74880869
Iteration 3181, loss = 1363.22346381
Iteration 3182, loss = 1362.83665531
Iteration 3183, loss = 1363.17274139
Iteration 3184, loss = 1362.58748352
Iteration 3185, loss = 1362.49485196
Iteration 3186, loss = 1362.52849649
Iteration 3187, loss = 1362.69802398
Iteration 3188, loss = 1362.67815443
Iteration 3189, loss = 1362.81375053
Iteration 3190, loss = 1362.55243569
Iteration 3191, loss = 1363.32961791
Iteration 3192, loss = 1362.40751070
Iteration 3193, loss = 1362.43819549
Iteration 3194, loss = 1362.49068382
Iteration 3195, loss = 1362.51827222
Iteration 3196, loss = 1362.30720900
Iteration 3197, loss = 1362.31188886
Iteration 3198, loss = 1362.31707442
Iteration 3199, loss = 1362.29038678
Iteration 3200, loss = 1362.76969174
Iteration 3201, loss = 1362.24720860
Iteration 3202, loss = 1362.43590285
Iteration 3203, loss = 1362.65361364
Iteration 3204, loss = 1362.27976900
Iteration 3205, loss = 1362.37117157
Iteration 3206, loss = 1362.50720482
Iteration 3207, loss = 1362.21214915
Iteration 3208, loss = 1362.17990117
Iteration 3209, loss = 1362.07632915
Iteration 3210, loss = 1362.10399806
Iteration 3211, loss = 1361.97956606
Iteration 3212, loss = 1361.96615294
Iteration 3213, loss = 1362.05085827
Iteration 3214, loss = 1361.92899139
Iteration 3215, loss = 1362.05960982
Iteration 3216, loss = 1362.62882661
Iteration 3217, loss = 1361.91385479
Iteration 3218, loss = 1361.85769412
Iteration 3219, loss = 1361.82579353
Iteration 3220, loss = 1361.83805242
Iteration 3221, loss = 1361.80904154
Iteration 3222, loss = 1361.75680893
Iteration 3223, loss = 1361.87336608
Iteration 3224, loss = 1361.86404676
Iteration 3225, loss = 1361.73175542
Iteration 3226, loss = 1362.71610396
Iteration 3227, loss = 1361.62881840
Iteration 3228, loss = 1361.91210889
Iteration 3229, loss = 1362.07470835
Iteration 3230, loss = 1361.71614908
Iteration 3231, loss = 1361.64692509
Iteration 3232, loss = 1361.65402229
Iteration 3233, loss = 1361.64534732
Iteration 3234, loss = 1362.55684387
Iteration 3235, loss = 1361.64395977
Iteration 3236, loss = 1361.55065617
Iteration 3237, loss = 1361.41961121
Iteration 3238, loss = 1362.03789884
Iteration 3239, loss = 1361.43210880
Iteration 3240, loss = 1361.48445525
Iteration 3241, loss = 1361.47146858
Iteration 3242, loss = 1361.36188327
Iteration 3243, loss = 1361.76687033
Iteration 3244, loss = 1361.57668383
Iteration 3245, loss = 1361.42296691
Iteration 3246, loss = 1361.26175592
Iteration 3247, loss = 1361.51217333
Iteration 3248, loss = 1361.36160824
Iteration 3249, loss = 1361.34095842
Iteration 3250, loss = 1361.14391163
Iteration 3251, loss = 1361.13451583
Iteration 3252, loss = 1361.40636629
Iteration 3253, loss = 1361.32586212
Iteration 3254, loss = 1361.06417959
Iteration 3255, loss = 1361.07565607
Iteration 3256, loss = 1361.20504534
Iteration 3257, loss = 1361.08265543
Iteration 3258, loss = 1361.04804891
Iteration 3259, loss = 1360.96505951
Iteration 3260, loss = 1360.95116324
Iteration 3261, loss = 1361.21047171
Iteration 3262, loss = 1360.93422988
Iteration 3263, loss = 1360.88655684
Iteration 3264, loss = 1360.96969728
Iteration 3265, loss = 1360.79134262
Iteration 3266, loss = 1360.82816858
Iteration 3267, loss = 1361.06521863
Iteration 3268, loss = 1360.86719405
Iteration 3269, loss = 1360.85919308
Iteration 3270, loss = 1360.71128786
Iteration 3271, loss = 1360.81257539
Iteration 3272, loss = 1360.88347114
Iteration 3273, loss = 1360.66510426
Iteration 3274, loss = 1360.61253019
Iteration 3275, loss = 1360.70731822
Iteration 3276, loss = 1360.59955657
Iteration 3277, loss = 1360.56258061
Iteration 3278, loss = 1360.96113177
Iteration 3279, loss = 1360.57026565
Iteration 3280, loss = 1360.93880688
Iteration 3281, loss = 1360.51091868
Iteration 3282, loss = 1360.50644922
Iteration 3283, loss = 1360.62552180
Iteration 3284, loss = 1360.42653020
Iteration 3285, loss = 1360.46051133
Iteration 3286, loss = 1361.32029499
Iteration 3287, loss = 1361.50946857
Iteration 3288, loss = 1360.81448643
Iteration 3289, loss = 1360.38815216
Iteration 3290, loss = 1360.59036643
Iteration 3291, loss = 1360.20570757
Iteration 3292, loss = 1360.19333042
Iteration 3293, loss = 1360.32602361
Iteration 3294, loss = 1360.45203186
Iteration 3295, loss = 1360.62026962
Iteration 3296, loss = 1360.89527521
Iteration 3297, loss = 1360.26657952
Iteration 3298, loss = 1360.31415383
Iteration 3299, loss = 1360.22239480
Iteration 3300, loss = 1360.42462794
Iteration 3301, loss = 1360.78301700
Iteration 3302, loss = 1360.12401439
Iteration 3303, loss = 1359.96776943
Iteration 3304, loss = 1359.97226114
Iteration 3305, loss = 1360.22491684
Iteration 3306, loss = 1360.35098182
Iteration 3307, loss = 1360.18462403
Iteration 3308, loss = 1360.19692322
Iteration 3309, loss = 1360.16248933
Iteration 3310, loss = 1360.55655641
Iteration 3311, loss = 1359.98798914
Iteration 3312, loss = 1360.07648404
Iteration 3313, loss = 1359.83195158
Iteration 3314, loss = 1359.85822950
Iteration 3315, loss = 1359.95590276
Iteration 3316, loss = 1360.11068908
Iteration 3317, loss = 1359.78080251
Iteration 3318, loss = 1359.77983165
Iteration 3319, loss = 1359.76068002
Iteration 3320, loss = 1359.68580709
Iteration 3321, loss = 1359.84354472
Iteration 3322, loss = 1359.67403814
Iteration 3323, loss = 1359.61926475
Iteration 3324, loss = 1359.71344972
Iteration 3325, loss = 1360.16575331
Iteration 3326, loss = 1359.62554271
Iteration 3327, loss = 1359.53751133
Iteration 3328, loss = 1359.63470708
Iteration 3329, loss = 1359.49527955
Iteration 3330, loss = 1359.52295939
Iteration 3331, loss = 1359.69581043
Iteration 3332, loss = 1359.62786684
Iteration 3333, loss = 1359.41471769
Iteration 3334, loss = 1359.60692871
Iteration 3335, loss = 1360.40938971
Iteration 3336, loss = 1359.84377744
Iteration 3337, loss = 1359.50323928
Iteration 3338, loss = 1359.60024202
Iteration 3339, loss = 1359.26115041
Iteration 3340, loss = 1359.24297672
Iteration 3341, loss = 1359.34699298
Iteration 3342, loss = 1359.65639348
Iteration 3343, loss = 1359.77651101
Iteration 3344, loss = 1359.61512601
Iteration 3345, loss = 1359.42496952
Iteration 3346, loss = 1359.19445061
Iteration 3347, loss = 1359.73473666
Iteration 3348, loss = 1359.44944326
Iteration 3349, loss = 1359.14306970
Iteration 3350, loss = 1359.48060403
Iteration 3351, loss = 1359.15182499
Iteration 3352, loss = 1359.45593388
Iteration 3353, loss = 1359.03801060
Iteration 3354, loss = 1359.34888662
Iteration 3355, loss = 1359.05412131
Iteration 3356, loss = 1359.06312543
Iteration 3357, loss = 1359.10410434
Iteration 3358, loss = 1359.13615871
Iteration 3359, loss = 1358.94585615
Iteration 3360, loss = 1358.87172473
Iteration 3361, loss = 1358.88943594
Iteration 3362, loss = 1358.83560215
Iteration 3363, loss = 1359.64969725
Iteration 3364, loss = 1358.82360444
Iteration 3365, loss = 1358.86308724
Iteration 3366, loss = 1358.85035102
Iteration 3367, loss = 1358.96271005
Iteration 3368, loss = 1358.74049755
Iteration 3369, loss = 1358.73406176
Iteration 3370, loss = 1359.36935694
Iteration 3371, loss = 1358.72197543
Iteration 3372, loss = 1359.38776292
Iteration 3373, loss = 1359.06800580
Iteration 3374, loss = 1359.29722972
Iteration 3375, loss = 1358.49711755
Iteration 3376, loss = 1358.53241401
Iteration 3377, loss = 1359.71033067
Iteration 3378, loss = 1358.74415027
Iteration 3379, loss = 1358.75382568
Iteration 3380, loss = 1358.76941199
Iteration 3381, loss = 1358.47837497
Iteration 3382, loss = 1358.58089253
Iteration 3383, loss = 1358.37180190
Iteration 3384, loss = 1358.72037794
Iteration 3385, loss = 1358.70653225
Iteration 3386, loss = 1358.47273557
Iteration 3387, loss = 1358.33219506
Iteration 3388, loss = 1358.68285346
Iteration 3389, loss = 1358.25639555
Iteration 3390, loss = 1358.29748035
Iteration 3391, loss = 1358.32005559
Iteration 3392, loss = 1358.63526816
Iteration 3393, loss = 1358.23663763
Iteration 3394, loss = 1358.27690133
Iteration 3395, loss = 1358.28600382
Iteration 3396, loss = 1358.24470100
Iteration 3397, loss = 1358.27360433
Iteration 3398, loss = 1358.04938849
Iteration 3399, loss = 1358.41221141
Iteration 3400, loss = 1358.57062547
Iteration 3401, loss = 1358.49110603
Iteration 3402, loss = 1358.18334432
Iteration 3403, loss = 1358.25419891
Iteration 3404, loss = 1358.00023958
Iteration 3405, loss = 1358.15400221
Iteration 3406, loss = 1357.95494997
Iteration 3407, loss = 1357.98775191
Iteration 3408, loss = 1358.18363714
Iteration 3409, loss = 1357.98943741
Iteration 3410, loss = 1358.27790376
Iteration 3411, loss = 1357.88874862
Iteration 3412, loss = 1357.86515645
Iteration 3413, loss = 1357.83078156
Iteration 3414, loss = 1357.86207281
Iteration 3415, loss = 1357.85279595
Iteration 3416, loss = 1357.67783070
Iteration 3417, loss = 1357.68778090
Iteration 3418, loss = 1357.98968440
Iteration 3419, loss = 1357.95128804
Iteration 3420, loss = 1357.77998740
Iteration 3421, loss = 1357.88237582
Iteration 3422, loss = 1357.61220705
Iteration 3423, loss = 1357.66346346
Iteration 3424, loss = 1357.52383487
Iteration 3425, loss = 1357.55182051
Iteration 3426, loss = 1357.63023566
Iteration 3427, loss = 1357.50077410
Iteration 3428, loss = 1357.43899917
Iteration 3429, loss = 1357.57702662
Iteration 3430, loss = 1357.74936966
Iteration 3431, loss = 1357.55263613
Iteration 3432, loss = 1357.35944836
Iteration 3433, loss = 1357.35550218
Iteration 3434, loss = 1357.42202273
Iteration 3435, loss = 1357.31258029
Iteration 3436, loss = 1357.56804362
Iteration 3437, loss = 1357.22184512
Iteration 3438, loss = 1357.31926835
Iteration 3439, loss = 1357.37160895
Iteration 3440, loss = 1357.18128612
Iteration 3441, loss = 1357.31508993
Iteration 3442, loss = 1357.22740469
Iteration 3443, loss = 1357.15485495
Iteration 3444, loss = 1357.12829001
Iteration 3445, loss = 1357.02926538
Iteration 3446, loss = 1357.46540223
Iteration 3447, loss = 1357.28315337
Iteration 3448, loss = 1357.33245074
Iteration 3449, loss = 1357.23596329
Iteration 3450, loss = 1357.20844472
Iteration 3451, loss = 1357.42236536
Iteration 3452, loss = 1356.98703459
Iteration 3453, loss = 1356.97852131
Iteration 3454, loss = 1356.92147019
Iteration 3455, loss = 1356.90078975
Iteration 3456, loss = 1356.83049896
Iteration 3457, loss = 1356.89748507
Iteration 3458, loss = 1357.01819849
Iteration 3459, loss = 1356.89519866
Iteration 3460, loss = 1356.86977975
Iteration 3461, loss = 1356.79413712
Iteration 3462, loss = 1357.02568260
Iteration 3463, loss = 1356.80547024
Iteration 3464, loss = 1356.70081763
Iteration 3465, loss = 1356.66761587
Iteration 3466, loss = 1356.90863328
Iteration 3467, loss = 1356.72748190
Iteration 3468, loss = 1356.60279778
Iteration 3469, loss = 1356.58344976
Iteration 3470, loss = 1356.80594291
Iteration 3471, loss = 1356.71775032
Iteration 3472, loss = 1357.00171381
Iteration 3473, loss = 1356.60623579
Iteration 3474, loss = 1357.73110050
Iteration 3475, loss = 1356.49061475
Iteration 3476, loss = 1356.50399992
Iteration 3477, loss = 1356.58382838
Iteration 3478, loss = 1356.49023414
Iteration 3479, loss = 1356.52281277
Iteration 3480, loss = 1356.55489963
Iteration 3481, loss = 1356.61236599
Iteration 3482, loss = 1356.59949577
Iteration 3483, loss = 1356.28781620
Iteration 3484, loss = 1356.26467116
Iteration 3485, loss = 1356.31798355
Iteration 3486, loss = 1356.44567594
Iteration 3487, loss = 1356.37251090
Iteration 3488, loss = 1356.29203180
Iteration 3489, loss = 1356.34323669
Iteration 3490, loss = 1356.44841290
Iteration 3491, loss = 1356.70018894
Iteration 3492, loss = 1356.34462371
Iteration 3493, loss = 1356.19947161
Iteration 3494, loss = 1356.69463142
Iteration 3495, loss = 1356.11026788
Iteration 3496, loss = 1356.14855203
Iteration 3497, loss = 1356.22795130
Iteration 3498, loss = 1356.16933493
Iteration 3499, loss = 1356.16662051
Iteration 3500, loss = 1356.17505230
Iteration 3501, loss = 1356.23333338
Iteration 3502, loss = 1355.92502397
Iteration 3503, loss = 1356.14755802
Iteration 3504, loss = 1355.90572451
Iteration 3505, loss = 1355.94715205
Iteration 3506, loss = 1355.97068307
Iteration 3507, loss = 1355.92635643
Iteration 3508, loss = 1356.49028486
Iteration 3509, loss = 1355.81788126
Iteration 3510, loss = 1355.86506187
Iteration 3511, loss = 1355.80018155
Iteration 3512, loss = 1355.76049608
Iteration 3513, loss = 1355.94688705
Iteration 3514, loss = 1355.66355449
Iteration 3515, loss = 1355.65854342
Iteration 3516, loss = 1355.68568955
Iteration 3517, loss = 1355.69631882
Iteration 3518, loss = 1355.70840545
Iteration 3519, loss = 1355.72104417
Iteration 3520, loss = 1355.66344452
Iteration 3521, loss = 1355.73392815
Iteration 3522, loss = 1355.55830532
Iteration 3523, loss = 1355.48864343
Iteration 3524, loss = 1355.70088903
Iteration 3525, loss = 1355.66021136
Iteration 3526, loss = 1355.49647601
Iteration 3527, loss = 1355.57543086
Iteration 3528, loss = 1356.35497116
Iteration 3529, loss = 1355.45332500
Iteration 3530, loss = 1355.60930230
Iteration 3531, loss = 1356.03703793
Iteration 3532, loss = 1355.51964487
Iteration 3533, loss = 1355.51910351
Iteration 3534, loss = 1355.74719904
Iteration 3535, loss = 1355.93680450
Iteration 3536, loss = 1355.30135697
Iteration 3537, loss = 1355.70460534
Iteration 3538, loss = 1356.03439061
Iteration 3539, loss = 1355.50579918
Iteration 3540, loss = 1355.22873420
Iteration 3541, loss = 1355.27568102
Iteration 3542, loss = 1355.18319940
Iteration 3543, loss = 1355.25301068
Iteration 3544, loss = 1355.20499552
Iteration 3545, loss = 1355.15765825
Iteration 3546, loss = 1355.34101198
Iteration 3547, loss = 1355.09737545
Iteration 3548, loss = 1355.12054807
Iteration 3549, loss = 1355.47921396
Iteration 3550, loss = 1355.08459927
Iteration 3551, loss = 1355.01688988
Iteration 3552, loss = 1355.02677953
Iteration 3553, loss = 1354.99835668
Iteration 3554, loss = 1355.25380003
Iteration 3555, loss = 1355.14433703
Iteration 3556, loss = 1354.89677982
Iteration 3557, loss = 1354.93220834
Iteration 3558, loss = 1355.32644372
Iteration 3559, loss = 1355.51478581
Iteration 3560, loss = 1355.72050160
Iteration 3561, loss = 1354.90019137
Iteration 3562, loss = 1354.89244794
Iteration 3563, loss = 1355.01254634
Iteration 3564, loss = 1354.89203175
Iteration 3565, loss = 1354.76779027
Iteration 3566, loss = 1354.73816903
Iteration 3567, loss = 1354.75628659
Iteration 3568, loss = 1354.70970204
Iteration 3569, loss = 1354.91715086
Iteration 3570, loss = 1354.62673373
Iteration 3571, loss = 1355.05439104
Iteration 3572, loss = 1354.63285290
Iteration 3573, loss = 1354.59638846
Iteration 3574, loss = 1354.62542526
Iteration 3575, loss = 1354.90366310
Iteration 3576, loss = 1354.51740736
Iteration 3577, loss = 1354.60322761
Iteration 3578, loss = 1354.40685073
Iteration 3579, loss = 1354.44796362
Iteration 3580, loss = 1354.85243303
Iteration 3581, loss = 1354.95604891
Iteration 3582, loss = 1355.05082969
Iteration 3583, loss = 1354.45245231
Iteration 3584, loss = 1354.47303835
Iteration 3585, loss = 1354.30437314
Iteration 3586, loss = 1354.27412854
Iteration 3587, loss = 1354.48659573
Iteration 3588, loss = 1354.46293082
Iteration 3589, loss = 1354.54882198
Iteration 3590, loss = 1354.40917438
Iteration 3591, loss = 1354.36217182
Iteration 3592, loss = 1354.29674122
Iteration 3593, loss = 1354.33813788
Iteration 3594, loss = 1354.32056844
Iteration 3595, loss = 1354.24759358
Iteration 3596, loss = 1354.20289188
Iteration 3597, loss = 1354.24838102
Iteration 3598, loss = 1354.15325833
Iteration 3599, loss = 1354.14960934
Iteration 3600, loss = 1355.15695611
Iteration 3601, loss = 1353.94940812
Iteration 3602, loss = 1353.99004523
Iteration 3603, loss = 1353.99422413
Iteration 3604, loss = 1354.38836273
Iteration 3605, loss = 1354.25972590
Iteration 3606, loss = 1354.35733766
Iteration 3607, loss = 1354.02713635
Iteration 3608, loss = 1354.00937187
Iteration 3609, loss = 1353.88956875
Iteration 3610, loss = 1353.89010129
Iteration 3611, loss = 1353.89856216
Iteration 3612, loss = 1354.09166879
Iteration 3613, loss = 1354.38277938
Iteration 3614, loss = 1353.79514514
Iteration 3615, loss = 1353.83834648
Iteration 3616, loss = 1354.20068967
Iteration 3617, loss = 1353.57290252
Iteration 3618, loss = 1354.09011564
Iteration 3619, loss = 1354.21943934
Iteration 3620, loss = 1354.15854569
Iteration 3621, loss = 1354.02813756
Iteration 3622, loss = 1353.77361166
Iteration 3623, loss = 1353.88320940
Iteration 3624, loss = 1353.68831307
Iteration 3625, loss = 1353.72506510
Iteration 3626, loss = 1353.79133533
Iteration 3627, loss = 1353.63237636
Iteration 3628, loss = 1353.66131947
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
numel for MLPRegressor = 11301
RMSE for MLP regressor for the diabetes dataset = 51.14554004371418
############## Training TensorBSplines Regressor ################
numel for TensorBSplines-Regressor = 110
TensorBSplines Regression Training:   0%|          | 0/10000 [00:00<?, ?it/s]i = 0,si = 28105.57719216265
TensorBSplines Regression Training:   1%|          | 101/10000 [00:13<15:09, 10.88it/s]i = 100,si = 18822.425567928152
TensorBSplines Regression Training:   2%|▏         | 201/10000 [00:22<14:17, 11.43it/s]i = 200,si = 11437.280883912037
TensorBSplines Regression Training:   3%|▎         | 301/10000 [00:31<14:00, 11.54it/s]i = 300,si = 7516.120269427755
TensorBSplines Regression Training:   4%|▍         | 401/10000 [00:39<13:59, 11.43it/s]i = 400,si = 6091.688398302234
TensorBSplines Regression Training:   5%|▌         | 501/10000 [00:48<14:12, 11.14it/s]i = 500,si = 5163.984093972153
TensorBSplines Regression Training:   6%|▌         | 601/10000 [00:57<13:38, 11.48it/s]i = 600,si = 4550.760782942848
TensorBSplines Regression Training:   7%|▋         | 701/10000 [01:06<13:30, 11.47it/s]i = 700,si = 4253.703355639839
TensorBSplines Regression Training:   8%|▊         | 801/10000 [01:15<13:21, 11.48it/s]i = 800,si = 3951.523817451216
TensorBSplines Regression Training:   9%|▉         | 901/10000 [01:23<13:15, 11.44it/s]i = 900,si = 3671.5841889094436
TensorBSplines Regression Training:  10%|█         | 1001/10000 [01:32<13:05, 11.46it/s]i = 1000,si = 3537.983756987203
TensorBSplines Regression Training:  11%|█         | 1101/10000 [01:41<12:51, 11.53it/s]i = 1100,si = 3234.9850777154484
TensorBSplines Regression Training:  12%|█▏        | 1201/10000 [01:50<12:40, 11.57it/s]i = 1200,si = 3320.396239775108
TensorBSplines Regression Training:  13%|█▎        | 1301/10000 [01:58<12:36, 11.50it/s]i = 1300,si = 3206.4902118424043
TensorBSplines Regression Training:  14%|█▍        | 1401/10000 [02:07<12:25, 11.54it/s]i = 1400,si = 2981.5160283125833
TensorBSplines Regression Training:  15%|█▌        | 1501/10000 [02:16<12:21, 11.46it/s]i = 1500,si = 2989.916598015254
TensorBSplines Regression Training:  16%|█▌        | 1601/10000 [02:24<12:06, 11.57it/s]i = 1600,si = 2975.091259266198
TensorBSplines Regression Training:  17%|█▋        | 1701/10000 [02:33<11:57, 11.56it/s]i = 1700,si = 2962.855269467438
TensorBSplines Regression Training:  18%|█▊        | 1801/10000 [02:42<11:45, 11.62it/s]i = 1800,si = 2862.106017961409
TensorBSplines Regression Training:  19%|█▉        | 1901/10000 [02:50<11:48, 11.43it/s]i = 1900,si = 2751.942159727435
TensorBSplines Regression Training:  20%|██        | 2001/10000 [02:59<11:30, 11.59it/s]i = 2000,si = 2684.839984381193
TensorBSplines Regression Training:  21%|██        | 2101/10000 [03:08<11:25, 11.53it/s]i = 2100,si = 2801.427029278786
TensorBSplines Regression Training:  22%|██▏       | 2201/10000 [03:16<11:18, 11.50it/s]i = 2200,si = 2643.500971958667
TensorBSplines Regression Training:  23%|██▎       | 2301/10000 [03:25<11:20, 11.32it/s]i = 2300,si = 2709.863558444273
TensorBSplines Regression Training:  24%|██▍       | 2401/10000 [03:34<10:59, 11.52it/s]i = 2400,si = 2814.14008670121
TensorBSplines Regression Training:  25%|██▌       | 2501/10000 [03:42<10:51, 11.52it/s]i = 2500,si = 2666.9133444794134
TensorBSplines Regression Training:  26%|██▌       | 2601/10000 [03:51<10:40, 11.56it/s]i = 2600,si = 2628.8212581534767
TensorBSplines Regression Training:  27%|██▋       | 2701/10000 [04:00<10:43, 11.34it/s]i = 2700,si = 2732.004612186121
TensorBSplines Regression Training:  28%|██▊       | 2801/10000 [04:09<10:32, 11.39it/s]i = 2800,si = 2647.0531699465414
TensorBSplines Regression Training:  29%|██▉       | 2901/10000 [04:17<10:13, 11.57it/s]i = 2900,si = 2787.868580627919
TensorBSplines Regression Training:  30%|███       | 3001/10000 [04:26<10:04, 11.58it/s]i = 3000,si = 2452.250268848565
TensorBSplines Regression Training:  31%|███       | 3101/10000 [04:35<10:00, 11.48it/s]i = 3100,si = 2538.723786036632
TensorBSplines Regression Training:  32%|███▏      | 3201/10000 [04:43<09:56, 11.41it/s]i = 3200,si = 2553.3838512038437
TensorBSplines Regression Training:  33%|███▎      | 3301/10000 [04:52<09:38, 11.58it/s]i = 3300,si = 2592.5263679805325
TensorBSplines Regression Training:  34%|███▍      | 3401/10000 [05:01<09:28, 11.61it/s]i = 3400,si = 2608.6029479601884
TensorBSplines Regression Training:  35%|███▌      | 3501/10000 [05:10<09:28, 11.44it/s]i = 3500,si = 2358.156538047078
TensorBSplines Regression Training:  36%|███▌      | 3601/10000 [05:18<09:15, 11.52it/s]i = 3600,si = 2624.254872572687
TensorBSplines Regression Training:  37%|███▋      | 3701/10000 [05:27<09:02, 11.60it/s]i = 3700,si = 2341.0892438266487
TensorBSplines Regression Training:  38%|███▊      | 3801/10000 [05:35<08:53, 11.62it/s]i = 3800,si = 2561.3509399532904
TensorBSplines Regression Training:  39%|███▉      | 3901/10000 [05:44<08:50, 11.50it/s]i = 3900,si = 2495.118108440391
TensorBSplines Regression Training:  40%|████      | 4001/10000 [05:53<08:37, 11.59it/s]i = 4000,si = 2629.3250336563624
TensorBSplines Regression Training:  41%|████      | 4101/10000 [06:01<08:28, 11.60it/s]i = 4100,si = 2573.4203333357204
TensorBSplines Regression Training:  42%|████▏     | 4201/10000 [06:10<08:30, 11.36it/s]i = 4200,si = 2665.298085546029
TensorBSplines Regression Training:  43%|████▎     | 4301/10000 [06:19<08:12, 11.58it/s]i = 4300,si = 2548.1778196071764
TensorBSplines Regression Training:  44%|████▍     | 4401/10000 [06:27<08:00, 11.65it/s]i = 4400,si = 2559.193535290387
TensorBSplines Regression Training:  45%|████▌     | 4501/10000 [06:36<07:55, 11.56it/s]i = 4500,si = 2580.1705776224558
TensorBSplines Regression Training:  46%|████▌     | 4601/10000 [06:45<08:06, 11.09it/s]i = 4600,si = 2441.5214962184314
TensorBSplines Regression Training:  47%|████▋     | 4701/10000 [06:53<07:41, 11.49it/s]i = 4700,si = 2562.5241670129267
TensorBSplines Regression Training:  48%|████▊     | 4801/10000 [07:02<07:31, 11.51it/s]i = 4800,si = 2411.4544737530696
TensorBSplines Regression Training:  49%|████▉     | 4901/10000 [07:11<07:20, 11.59it/s]i = 4900,si = 2574.983401860613
TensorBSplines Regression Training:  50%|█████     | 5001/10000 [07:20<07:16, 11.44it/s]i = 5000,si = 2644.1566794934756
TensorBSplines Regression Training:  51%|█████     | 5101/10000 [07:28<07:06, 11.50it/s]i = 5100,si = 2482.1278554979667
TensorBSplines Regression Training:  52%|█████▏    | 5201/10000 [07:37<06:55, 11.55it/s]i = 5200,si = 2376.1490373938345
TensorBSplines Regression Training:  53%|█████▎    | 5301/10000 [07:46<06:47, 11.52it/s]i = 5300,si = 2518.038333554313
TensorBSplines Regression Training:  54%|█████▍    | 5401/10000 [07:54<06:47, 11.29it/s]i = 5400,si = 2428.775207623189
TensorBSplines Regression Training:  55%|█████▌    | 5501/10000 [08:03<06:30, 11.53it/s]i = 5500,si = 2449.557343081915
TensorBSplines Regression Training:  56%|█████▌    | 5601/10000 [08:12<06:19, 11.59it/s]i = 5600,si = 2491.190600059277
TensorBSplines Regression Training:  57%|█████▋    | 5701/10000 [08:20<06:12, 11.55it/s]i = 5700,si = 2463.331648096283
TensorBSplines Regression Training:  58%|█████▊    | 5801/10000 [08:29<06:08, 11.39it/s]i = 5800,si = 2439.802526893602
TensorBSplines Regression Training:  59%|█████▉    | 5901/10000 [08:38<05:54, 11.55it/s]i = 5900,si = 2397.3478789724677
TensorBSplines Regression Training:  60%|██████    | 6001/10000 [08:47<05:45, 11.59it/s]i = 6000,si = 2395.899228090348
TensorBSplines Regression Training:  61%|██████    | 6101/10000 [08:55<05:37, 11.55it/s]i = 6100,si = 2404.838469725103
TensorBSplines Regression Training:  62%|██████▏   | 6201/10000 [09:04<05:30, 11.51it/s]i = 6200,si = 2514.917274197934
TensorBSplines Regression Training:  63%|██████▎   | 6301/10000 [09:13<05:22, 11.48it/s]i = 6300,si = 2497.94881723227
TensorBSplines Regression Training:  64%|██████▍   | 6401/10000 [09:21<05:11, 11.56it/s]i = 6400,si = 2304.7330916689775
TensorBSplines Regression Training:  65%|██████▌   | 6501/10000 [09:30<05:02, 11.57it/s]i = 6500,si = 2390.744190220642
TensorBSplines Regression Training:  66%|██████▌   | 6601/10000 [09:39<04:58, 11.40it/s]i = 6600,si = 2453.4745730002483
TensorBSplines Regression Training:  67%|██████▋   | 6701/10000 [09:47<04:45, 11.54it/s]i = 6700,si = 2353.8277676937532
TensorBSplines Regression Training:  68%|██████▊   | 6801/10000 [09:56<04:37, 11.52it/s]i = 6800,si = 2304.3638834296694
TensorBSplines Regression Training:  69%|██████▉   | 6901/10000 [10:05<04:29, 11.49it/s]i = 6900,si = 2513.764808256418
TensorBSplines Regression Training:  70%|███████   | 7000/10000 [10:14<04:19, 11.55it/s]i = 7000,si = 2460.681523124774
TensorBSplines Regression Training:  71%|███████   | 7100/10000 [10:22<04:11, 11.53it/s]i = 7100,si = 2374.956481703138
TensorBSplines Regression Training:  72%|███████▏  | 7200/10000 [10:31<04:01, 11.60it/s]i = 7200,si = 2368.425104683527
TensorBSplines Regression Training:  73%|███████▎  | 7300/10000 [10:40<03:54, 11.54it/s]i = 7300,si = 2390.801442177319
TensorBSplines Regression Training:  74%|███████▍  | 7400/10000 [10:49<03:45, 11.52it/s]i = 7400,si = 2463.4384874238267
TensorBSplines Regression Training:  75%|███████▌  | 7500/10000 [10:57<03:36, 11.56it/s]i = 7500,si = 2221.7158285971523
TensorBSplines Regression Training:  76%|███████▌  | 7600/10000 [11:06<03:27, 11.56it/s]i = 7600,si = 2653.950398964846
TensorBSplines Regression Training:  77%|███████▋  | 7700/10000 [11:15<03:19, 11.50it/s]i = 7700,si = 2413.6158211658576
TensorBSplines Regression Training:  78%|███████▊  | 7800/10000 [11:23<03:14, 11.33it/s]i = 7800,si = 2432.8549110844156
TensorBSplines Regression Training:  79%|███████▉  | 7900/10000 [11:32<03:01, 11.56it/s]i = 7900,si = 2361.6210892944155
TensorBSplines Regression Training:  80%|████████  | 8000/10000 [11:41<02:51, 11.67it/s]i = 8000,si = 2572.436440096552
TensorBSplines Regression Training:  81%|████████  | 8100/10000 [11:49<02:46, 11.42it/s]i = 8100,si = 2454.9238057726047
TensorBSplines Regression Training:  82%|████████▏ | 8200/10000 [11:58<02:40, 11.25it/s]i = 8200,si = 2538.6678885161855
TensorBSplines Regression Training:  83%|████████▎ | 8300/10000 [12:07<02:29, 11.37it/s]i = 8300,si = 2497.8151447344558
TensorBSplines Regression Training:  84%|████████▍ | 8400/10000 [12:16<02:19, 11.47it/s]i = 8400,si = 2335.4595169838253
TensorBSplines Regression Training:  85%|████████▌ | 8500/10000 [12:25<02:13, 11.20it/s]i = 8500,si = 2326.9909930303374
TensorBSplines Regression Training:  86%|████████▌ | 8600/10000 [12:34<02:02, 11.39it/s]i = 8600,si = 2400.08511020546
TensorBSplines Regression Training:  87%|████████▋ | 8700/10000 [12:42<01:53, 11.44it/s]i = 8700,si = 2382.678064190003
TensorBSplines Regression Training:  88%|████████▊ | 8800/10000 [12:51<01:45, 11.41it/s]i = 8800,si = 2440.451912952733
TensorBSplines Regression Training:  89%|████████▉ | 8900/10000 [13:00<01:36, 11.43it/s]i = 8900,si = 2392.4913460601115
TensorBSplines Regression Training:  90%|█████████ | 9000/10000 [13:09<01:28, 11.32it/s]i = 9000,si = 2478.2079964927
TensorBSplines Regression Training:  91%|█████████ | 9100/10000 [13:17<01:19, 11.37it/s]i = 9100,si = 2536.1536367521435
TensorBSplines Regression Training:  92%|█████████▏| 9200/10000 [13:26<01:10, 11.42it/s]i = 9200,si = 2302.875481095161
TensorBSplines Regression Training:  93%|█████████▎| 9300/10000 [13:35<01:01, 11.43it/s]i = 9300,si = 2350.757608512369
TensorBSplines Regression Training:  94%|█████████▍| 9400/10000 [13:44<00:52, 11.43it/s]i = 9400,si = 2331.0695977614737
TensorBSplines Regression Training:  95%|█████████▌| 9500/10000 [13:53<00:43, 11.45it/s]i = 9500,si = 2499.539709072924
TensorBSplines Regression Training:  96%|█████████▌| 9600/10000 [14:01<00:35, 11.37it/s]i = 9600,si = 2332.374273238058
TensorBSplines Regression Training:  97%|█████████▋| 9700/10000 [14:10<00:26, 11.50it/s]i = 9700,si = 2333.3219914389147
TensorBSplines Regression Training:  98%|█████████▊| 9800/10000 [14:19<00:17, 11.41it/s]i = 9800,si = 2297.0278909250605
TensorBSplines Regression Training:  99%|█████████▉| 9900/10000 [14:28<00:08, 11.39it/s]i = 9900,si = 2325.3491610397973
TensorBSplines Regression Training: 100%|██████████| 10000/10000 [14:37<00:00, 11.40it/s]
RMSE for TensorBSplines regressor for the diabetes dataset = 54.926424384406886

Process finished with exit code 0
